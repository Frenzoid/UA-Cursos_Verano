{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxjUB7HnAnLD6nzCAVupiS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ejercicio de regresión\n","\n","En este ejercicio se pide que cargues la base de datos [California Housing](https://www.kaggle.com/datasets/darshanprabhu09/california-housing-dataset) sobre precios de casas en California y que escribas código para realizar las siguientes tareas:\n","\n","1. Realices la carga de datos de la base de datos y pre-proceses los datos para normalizar las características predictivas.\n","\n","2. Crees unas particiones de *train* y *test*.\n","\n","3. Entrenes un regresor lineal.\n","\n","4. Implementes una figura de mérito para evaluar la bondad del método.\n","\n","5. Estudies la influencia del parámetro *k* que define la cantidad de vecinos a utilizar.\n","\n","\n","Para evaluar la bondad del método utilizarás el \"error absoluto promedio\" (en inglés, *Mean Absolute Error* o *MAE*). Para un conjunto de datos, esta métrica se define como:\n","\n","$ \\mbox{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}{\\left|y[i] - \\hat{y}[i]\\right|} $\n","\n","done $n$ representa la cantidad de muestras, $y$ los valores esperados e $\\hat{y}$ el vector de predicciones.\n","<br>\n","\n","### Especificaciones:\n","* La base de datos California Housing será la que incluye Google Colab en la ruta '/content/sample_data/california_housing_test.csv'.\n"," * El valor a predecir es la columna *median_house_value*.\n"," * El resto de columnas representa las características.\n","* Las características deberán normalizarse de manera que queden en el rango [0,1] (método `MinMaxScaler()` de `sklearn`).\n","* La función que calcule la tasa de acierto deberá llamarse `MAE` y tener como parámetros las clases objetivo (`y_true`) y las clases predichas (`y_pred`), es decir: `def MAE(y_true, y_pred)`\n","\n","<br>"],"metadata":{"id":"qmmPpiN8BqbD"}},{"cell_type":"markdown","source":["En primera instancia realizamos la carga de datos y dividimos los datos entre entrenamiento y mostramos la tabla para tener cierta idea de cómo son:"],"metadata":{"id":"Btk12hcMSNT2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKbXGtNBAbOs"},"outputs":[],"source":["# Añade aquí el código\n","import pandas as pd\n","import numpy as np\n","\n","# Cargamos los datos y los mostramos:\n","data = pd.read_csv('/content/sample_data/california_housing_test.csv')\n","display(data)"]},{"cell_type":"markdown","source":["Separamos ahora entre atributos (X) y objetivo (Y):"],"metadata":{"id":"0IXZTs08xAg9"}},{"cell_type":"code","source":["# Separamos entre atributos y valor esperado, y transformamos a Numpy:\n","Y = np.asarray( data.loc[:,'median_house_value'] )\n","X = np.asarray( data.loc[:,'longitude':'median_income'] )"],"metadata":{"id":"8iHptOQA-A8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aplicamos el proceso de normalización de datos:"],"metadata":{"id":"sf7YeL6CxK5V"}},{"cell_type":"code","source":["# Normalizamos los datos:\n","from sklearn.preprocessing import MinMaxScaler\n","\n","sc = MinMaxScaler()\n","X_scaled = sc.fit_transform(X)"],"metadata":{"id":"UIVCfZIa_tKW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Realizamos las particiones de *train* y *test*:"],"metadata":{"id":"M5jOqX9_xORJ"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(X_scaled, Y,\n","                                                    test_size=0.2,\n","                                                    random_state=1,\n","                                                    shuffle=True)"],"metadata":{"id":"K-l1N3X-_uAg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definimos la métrica MAE:"],"metadata":{"id":"5-xiTxvbxaWt"}},{"cell_type":"code","source":["def MAE(y_true, y_pred):\n","  sum = 0\n","\n","  for idx in range(y_pred.shape[0]):\n","    sum += abs(y_pred[idx] - y_true[idx])\n","\n","  return sum/y_pred.shape[0]"],"metadata":{"id":"4bahj4wAxToU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Importamos el modelo de regresión y lo entrenamos:"],"metadata":{"id":"Z0Swhwpcxcy2"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","model = LinearRegression()\n","model.fit(x_train, y_train)"],"metadata":{"id":"S6HMS55Z_Td-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Realizamos la predicción con el modelo entrenado y mostramos el error (MAE) obtenido:"],"metadata":{"id":"CXc2m6hIxkRk"}},{"cell_type":"code","source":["# Obtenemos la predicción:\n","y_pred = model.predict(x_test)\n","\n","# Calculamos el valor de MAE:\n","print(\"MAE: {}\".format(MAE(y_true = y_test, y_pred = y_pred)))"],"metadata":{"id":"a3ZAdKzuAAxf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-------\n","Propuesta de trabajo **futuro**: utilizar modelos regularizados como *Lasso*, *Ridge* o *ElasticSearch*"],"metadata":{"id":"eysnPcbFyOBZ"}}]}