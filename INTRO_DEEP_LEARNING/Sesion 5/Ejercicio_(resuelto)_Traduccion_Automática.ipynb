{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXW9rnD8km0"
      },
      "source": [
        "# Ejercicio (resuelto) - Traduccion Automática"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hD1h2-Y8szlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYGWCLFVuuRp"
      },
      "source": [
        "En esta sesión veremos una arquitectura secuencia a secuencia (sequence-to-sequence, o *seq2seq*) para traducción neuronal de inglés a español. Implementaremos la red neuronal siguiente una arquitectura encoder-decoder. La estructura se ilustra en la siguiente imagen:\n",
        "\n",
        "![texto alternativo](https://docs.google.com/uc?id=189LP5y8Q2ocge971-nzUUXQb4yFzDdyr)\n",
        "\n",
        "\n",
        "Como se puede apreciar, vamos a realizar la traducción **a nivel de caracter**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1k4h-AFY_vM"
      },
      "source": [
        "## Código Keras paso a paso\n",
        "\n",
        "Para entrenar un traductor automático neuronal, sólo necesitamos frases en el idioma origen y su correspondiente traducción en el idioma destino. A diferencia de otro tipo de sistemas, no es necesario proporcionar un alineamiento entre los elementos de cada frase.\n",
        "\n",
        "Para este ejemplo vamos a utilizar el corpus que se proporciona en [este enlace](http://www.dlsi.ua.es/~jcalvo/data/eng-spa.txt). Este corpus se ha extraído de [este repositorio](http://www.manythings.org/anki/), con diversas opciones para otros pares de idiomas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5GB4PkniBl6"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "# Número de pares de secuencias que vamos a considerar (aumentalo para un mejor entrenamiento)\n",
        "limit_of_sentences = 4000\n",
        "\n",
        "# Se descarga el bitexto (pares inglés-español) y se descompone en lineas\n",
        "bitext_url = \"http://www.dlsi.ua.es/~jcalvo/data/eng-spa.txt\"\n",
        "response = urllib.request.urlopen(bitext_url)\n",
        "bibtext = response.read().decode('utf-8').splitlines()\n",
        "\n",
        "# Creamos los corpus de idioma origen y destino\n",
        "data_input = []\n",
        "data_output = []\n",
        "\n",
        "# Creamos los pares de datos limitados por el número de secuencias escogido\n",
        "for idx in range( min(len(bibtext), limit_of_sentences) ):\n",
        "    english, spanish = bibtext[idx].split('\\t')\n",
        "    data_input.append( english )\n",
        "    data_output.append( spanish )\n",
        "\n",
        "# Comprobación\n",
        "print('Frase ejemplo inglés: {}'.format(data_input[3]))\n",
        "print('Frase ejemplo español: {}'.format(data_output[3]))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMJQaGN2j1lx"
      },
      "source": [
        "Los datos vienen ordenados por longitud y frase: la red podría utilizar el orden para *ahorrarse* información en el vector de contexto, lo cual podría ser perjudicial para el aprendizaje. Así pues, barajamos los datos manteniendo el alineamiento con el *ground-truth*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zAApLkQj2GW"
      },
      "source": [
        "import random\n",
        "\n",
        "# Enlazamos ambas listas\n",
        "common_set = list(zip(data_input, data_output))\n",
        "\n",
        "# Se barajan y se desenlazan de nuevo\n",
        "random.shuffle(common_set)\n",
        "data_input[:], data_output[:] = zip(*common_set)\n",
        "\n",
        "# Comprobación\n",
        "print('Frase ejemplo inglés: {}'.format(data_input[3]))\n",
        "print('Frase ejemplo español: {}'.format(data_output[3]))\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWqxjoZHZJ9K"
      },
      "source": [
        "En este punto vamos a incorporar los caracteres de comienzo de secuencia (start of sentence, *sos*) y final de secuencia (end of sentence, *eos*). Vamos a utilizar dos caracteres especiales para indicarlos, con atención a que éstos no pertenezcan a los alfabetos de salida.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFuhh12gZQo4"
      },
      "source": [
        "# Caracteres de comienzo y final\n",
        "output_sos = '<'\n",
        "output_eos = '>'\n",
        "\n",
        "# Las datos de salida se transforman para incluir ambos\n",
        "data_output = [output_sos + output_sentence + output_eos for output_sentence in data_output]\n",
        "\n",
        "# Comprobación\n",
        "print('Frase ejemplo inglés: {}'.format(data_input[3]))\n",
        "print('Frase ejemplo español: {}'.format(data_output[3]))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwx74kgoZ5Rx"
      },
      "source": [
        "Ahora necesitamos conocer los alfabetos de entrada y salida, especialmente su longitud. Esta información es necesaria para saber la dimension de la representación one hot a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUuepHIiZ9WY"
      },
      "source": [
        "# Conjuntos de caracteres\n",
        "input_alphabet = set()\n",
        "output_alphabet = set()\n",
        "\n",
        "for input_sentence in data_input:\n",
        "    input_alphabet.update(list(input_sentence))\n",
        "\n",
        "for output_sentence in data_output:\n",
        "    output_alphabet.update(list(output_sentence))\n",
        "\n",
        "# Guardamos sus longitudes en variables\n",
        "input_alphabet_len = len(input_alphabet)\n",
        "output_alphabet_len = len(output_alphabet)\n",
        "\n",
        "# Comprobación\n",
        "print('Alfabeto de entrada de ' + str(input_alphabet_len) + ' caracteres: ' + str(input_alphabet))\n",
        "print('Alfabeto de salida de ' + str(output_alphabet_len) + ' caracteres: ' + str(output_alphabet))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPB7eNRhaLt_"
      },
      "source": [
        "Asignamos un valor numérico identificativo a cada caracter, guardando la conversión entre ambas representaciones para el futuro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV8vU7bEaWpR"
      },
      "source": [
        "# De caracter a entero\n",
        "input_alphabet_from_char_to_int = dict([(char, i) for i, char in enumerate(input_alphabet)])\n",
        "output_alphabet_from_char_to_int = dict([(char, i) for i, char in enumerate(output_alphabet)])\n",
        "\n",
        "# De entero a caracter\n",
        "input_alphabet_from_int_to_char = dict([(i, char) for i, char in enumerate(input_alphabet)])\n",
        "output_alphabet_from_int_to_char = dict([(i, char) for i, char in enumerate(output_alphabet)])\n",
        "\n",
        "# Comprobación\n",
        "print('Identificador 0 para el vocabulario de entrada: ' + str(input_alphabet_from_int_to_char[0]))\n",
        "print('Identificador de \\'a\\' para el vocabulario de entrada: ' + str(input_alphabet_from_char_to_int['a']))\n",
        "print('')\n",
        "print('Identificador 0 para el vocabulario de salida: ' + str(output_alphabet_from_int_to_char[0]))\n",
        "print('Identificador de \\'a\\' para el vocabulario de salida: ' + str(output_alphabet_from_char_to_int['a']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp6pw64dePen"
      },
      "source": [
        "Construimos los generadores que se usarán para alimentar a la red. Debe tenerse en cuenta que hay que hacer *padding* dentro de cada paquete creado, ya que los tensores a devolver deben tener una dimensión definida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP3RLK8UeUNn"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Función auxiliar que convierte listas en paquetes de datos\n",
        "def batch_to_vectors(data_input, data_output):\n",
        "  max_input_len = max([len(input_sentence) for input_sentence in data_input])\n",
        "  max_output_len = max([len(output_sentence) for output_sentence in data_output])\n",
        "\n",
        "  encoder_input = np.zeros((len(data_input),max_input_len,input_alphabet_len), dtype=np.float)\n",
        "\n",
        "  for idx_s, input_sentence in enumerate(data_input):\n",
        "      for idx_c, char in enumerate(input_sentence):\n",
        "          encoder_input[idx_s][idx_c][input_alphabet_from_char_to_int[char]] = 1.\n",
        "\n",
        "\n",
        "  decoder_input = np.zeros((len(data_input),max_output_len,output_alphabet_len), dtype=np.float)\n",
        "  decoder_output = np.zeros((len(data_input),max_output_len,output_alphabet_len), dtype=np.float)\n",
        "\n",
        "  for idx_s, output_sentence in enumerate(data_output):\n",
        "      for idx_c, char in enumerate(output_sentence):\n",
        "          decoder_input[idx_s][idx_c][output_alphabet_from_char_to_int[char]] = 1.\n",
        "          if idx_c > 0:\n",
        "              decoder_output[idx_s][idx_c-1][output_alphabet_from_char_to_int[char]] = 1.\n",
        "\n",
        "  return encoder_input, decoder_input, decoder_output\n",
        "\n",
        "# Función generadora\n",
        "def generator(data_input_full, data_output_full, batch_size=16):\n",
        "  while True:\n",
        "    for idx in range(0,len(data_input_full), batch_size):\n",
        "      data_input = data_input_full[idx:idx+batch_size]\n",
        "      data_output = data_output_full[idx:idx+batch_size]\n",
        "\n",
        "      x,y,t = batch_to_vectors(data_input,data_output)\n",
        "      yield [x,y], t\n",
        "\n",
        "\n",
        "# Inicializamos un generador para la comprobación\n",
        "check_generator = generator(data_input, data_output)\n",
        "\n",
        "for _ in range(5):\n",
        "  [encoder_input, decoder_input], decoder_output = next(check_generator)\n",
        "  print ('')\n",
        "  print (encoder_input.shape)\n",
        "  print (decoder_input.shape)\n",
        "  print (decoder_output.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iYDNV3edBh"
      },
      "source": [
        "Ahora vamos a construir el model neuronal. Cosas a tener en cuenta:\n",
        "*  ```return_sequences=True``` hace que nos devuelva las predicciones en cada paso.\n",
        "* ``` return_states=True``` hace que se devuelvan los estados internos.\n",
        "* Como vamos a utilizar celdas LSTM, los estados internos se componen tanto del estado interno en sí (h) como de la celda memoria (c)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iShWBS13ehPB"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None, input_alphabet_len), name='encoder_input')\n",
        "encoder_outputs, encoder_h, encoder_c = LSTM(256, return_state=True, name='encoder_output')(encoder_inputs)\n",
        "context_vector = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None, output_alphabet_len), name='decoder_input')\n",
        "decoder_outputs, _, _ = LSTM(256, return_sequences=True, return_state=True, name='decoder_lstm')(decoder_inputs, initial_state=context_vector)\n",
        "decoder_outputs = Dense(output_alphabet_len, activation='softmax',name='decoder_output')(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhnjH-8RgeD9"
      },
      "source": [
        "## Fase de entrenamiento\n",
        "\n",
        "Vamos a utilizar 1 % del corpus para validar, dejando el 99 % para entrenar la red.\n",
        "\n",
        "Para la función de entrenamiento, creamos un generador para el entrenamiento y otro para la validación. No obstante, también obtendremos el paquete completo de validación para monitorizar visualmente qué ocurre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRoG_z1rtpQL"
      },
      "source": [
        "val_split = 0.01\n",
        "idx_split = int(len(data_input)*val_split)\n",
        "\n",
        "data_input_train = data_input[idx_split:]\n",
        "data_output_train = data_output[idx_split:]\n",
        "\n",
        "data_input_validation = data_input[:idx_split]\n",
        "data_output_validation = data_output[:idx_split]\n",
        "\n",
        "training_generator = generator(data_input_train, data_output_train)\n",
        "validation_generator = generator(data_input_validation, data_output_validation)\n",
        "\n",
        "x_val, y_val, t_val = batch_to_vectors(data_input_validation,data_output_validation)\n",
        "\n",
        "print('Tamaño del conjunto de validación: ' + str(x_val.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDThbVMttoCm"
      },
      "source": [
        "Entrenamos con los generadores, para lo cual hay que especificar cuántos pasos corresponden a una época: número de datos dividido por el tamaño de los paquetes de datos. Tras cada época, vamos a comprobar qué resultados de traducción proporciona la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PTzl8lIg-Ui"
      },
      "source": [
        "import random\n",
        "\n",
        "# Entrenamiento durante 50 épocas con super-épocas\n",
        "for epoch in range(50):\n",
        "    print('Epoca ' + str(epoch))\n",
        "\n",
        "    # Entrenamos durante una época\n",
        "    model.fit_generator(training_generator,\n",
        "                       steps_per_epoch=len(data_input_train)//16,\n",
        "                       verbose=2,\n",
        "                       epochs=1,\n",
        "                       validation_data=[[x_val,y_val],t_val])\n",
        "\n",
        "    # Predecimos sobre el paquete de validación\n",
        "    batch_prediction = model.predict([x_val,y_val],batch_size=16)\n",
        "\n",
        "    # ---------------------------------------------\n",
        "    # Comprobamos la traduccion de alguna secuencia\n",
        "    idx_val_sentence = random.randint(0,x_val.shape[0]-1)\n",
        "    sentence_prediction = batch_prediction[idx_val_sentence]\n",
        "\n",
        "    # Obtenemos la frase predicha\n",
        "    raw_predicted_sequence = [output_alphabet_from_int_to_char[char] for char in np.argmax(sentence_prediction,axis=1)]\n",
        "\n",
        "    # Leemos solo hasta el caracter EOS\n",
        "    predicted_sentence = output_sos\n",
        "    for char in raw_predicted_sequence:\n",
        "        predicted_sentence += char\n",
        "        if char == output_eos:\n",
        "            break\n",
        "\n",
        "    # Resultado\n",
        "    print( 'Entrada:\\t' + str(data_input_validation[idx_val_sentence]))\n",
        "    print( 'Predicción:\\t' + str(predicted_sentence) )\n",
        "    print( 'Sal. esperada:\\t' + str(data_output_validation[idx_val_sentence]) )\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeh8yLbNxpBj"
      },
      "source": [
        "### Consideraciones\n",
        "\n",
        "La traducción entre lenguajes complejos como el inglés y el español es dificil de modelar. Los resultados obtenidos en este ejemplo están lejos de la fiabilidad que alcanzan los esquemas actuales. No obstante, la formulación de los modelos del estado del arte son similares: la diferencia rádica especialmente en la capacidad de cómputo y la cantidad de datos (en este ejemplo es sencillo paliar esto último pero provoca una mayor necesidad de recursos computacionales).\n",
        "\n",
        "No obstante, existen otras cuestiones que hemos obviado en este ejercicio como la búsqueda de hiper-parámetros adecuados, modelos de lenguaje, (word) embeddings o modelos de atención."
      ]
    }
  ]
}