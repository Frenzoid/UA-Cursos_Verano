{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEIQQXr_-dsr"
      },
      "source": [
        "# Ejercicio (resuelto) - Reconocimiento de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwNKIv9ynJv1"
      },
      "source": [
        "En esta sesión implementaremos un reconocedor de texto (*Optical Character Recognition*, OCR) mediante un enfoque encoder-decoder. La arquitectura a implementar seguirá una estructura como la descrita en la siguiente imágen:\n",
        "\n",
        "![Arquitectura](https://drive.google.com/uc?id=1nti512wD3j5xUuzlHh8GdOfxs7jq2bf7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1X7sOxxnLou"
      },
      "source": [
        "### Código Keras paso a paso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMgZKYXvXnYV"
      },
      "source": [
        "En primer lugar vamos a implementar una función que nos genere una imagen con el texto que se le pasa por parámetro. Para ello utilizaremos la librería OpenCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d87AcOomnHw4"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Función que genera una imagen con el texto pasado por parámetro\n",
        "def generateText(text):\n",
        "    # Caracteristicas del texto a generar\n",
        "    font = cv2.FONT_HERSHEY_SCRIPT_SIMPLEX # Fuente que simula escritura manuscrita\n",
        "    font_scale = 4\n",
        "    margin = 100\n",
        "    thickness = 4\n",
        "    color = 255\n",
        "\n",
        "    # Calculamos lo que ocuparía el texto con las características anteriores\n",
        "    size = cv2.getTextSize(text, font, font_scale, thickness)\n",
        "    text_width = size[0][0]\n",
        "    text_height = size[0][1]\n",
        "\n",
        "    # Creamos una imagen vacía\n",
        "    image = np.zeros((text_height+margin,text_width+margin*2),'uint8')\n",
        "\n",
        "    # Calculamos la posición donde poner el texto\n",
        "    x = margin\n",
        "    y = (image.shape[0] + text_height) // 2\n",
        "\n",
        "    # Pintamos el texto en la imagen\n",
        "    cv2.putText(image, text, (x, y), font, font_scale, color, thickness)\n",
        "\n",
        "    # Devolvemos la imagen\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLZJzGornzoY"
      },
      "source": [
        "Podemos comprobar qué aspecto tienen las imagenes generadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuEBaFixn4VI"
      },
      "source": [
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.dpi']= 150\n",
        "\n",
        "\n",
        "# Generamos una imagen con el texto `Deep Learning`\n",
        "texto_img = generateText(\"Deep Learning\")\n",
        "\n",
        "# Mostramos la imagen con matplotlib\n",
        "pyplot.imshow(texto_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiThkWQkoYRh"
      },
      "source": [
        "Con esta función podemos generar las imágenes pero nos falta saber qué vocabulario vamos a considerar. En lugar de generar palabras aleatorias, vamos a utilizar conjuntos de palabras pre-establecidos. En concreto, vamos a crear *nombres propios* a partir de palabras en mayúsculas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A0ByuJMopSZ"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "# Descargamos un vocabulario de la web (como en la Sesion 3.1.1)\n",
        "word_url = \"http://svnweb.freebsd.org/csrg/share/dict/words?view=co&content-type=text/plain\"\n",
        "words = urllib.request.urlopen(word_url).read().decode().splitlines()\n",
        "\n",
        "# Filtramos las palabras que solo tengan minusculas\n",
        "words  = [word for word in words if word.islower()]\n",
        "\n",
        "# Comprobación\n",
        "print('Num. palabras: {}'.format(len(words)))\n",
        "print('Palabra (0): {}'.format(words[0]))\n",
        "print('Palabra (100): {}'.format(words[100]))\n",
        "print('Palabra (1000): {}'.format(words[1000]))\n",
        "print('Palabra (10000): {}'.format(words[10000]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVj0b62bq2aQ"
      },
      "source": [
        "Nuestro conjunto de entrenamiento va a estar compuesto por muestras que contienen dos palabras de este conjunto (como si fueran un nombre y un apellido). Seleccionaremos al azar 5000 combinaciones para obtener nuestro conjunto de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuCOmk6FrDfP"
      },
      "source": [
        "import random\n",
        "\n",
        "# Limite del conjunto\n",
        "training_set_size = 5000\n",
        "\n",
        "# Par de entrenamiento\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "# Escogemos `training_set_size` palabras al azar\n",
        "for n in range(training_set_size):\n",
        "    Y.append(random.choice(words))\n",
        "\n",
        "# Creamos las correspondientes imagenes\n",
        "for name in Y:\n",
        "    X.append(generateText(name))\n",
        "\n",
        "# Comprobación\n",
        "print('Tamaño del conjunto de entrenamiento (X): {}'.format(len(X)))\n",
        "print('Tamaño del conjunto de entrenamiento (Y): {}'.format(len(Y)))\n",
        "\n",
        "# Ejemplo de muestra\n",
        "pyplot.imshow(X[0])                         # Pintar por pantalla la imagen (X)\n",
        "print('Ejemplo de Y (0): {}'.format(Y[0]))  # Imprimir la palabra en texto (Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_i7b2Rmjaq3"
      },
      "source": [
        "Los datos vienen ordenados alfabéticamente: la red podría utilizar el orden para *ahorrarse* información en el vector de contexto, lo cual podría ser perjudicial para el aprendizaje. Así pues, barajamos los datos manteniendo el alineamiento con el *ground-truth*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kosf-ia0ja0k"
      },
      "source": [
        "import random\n",
        "\n",
        "# Enlazamos ambas listas\n",
        "common_set = list(zip(X, Y))\n",
        "\n",
        "# Se barajan y se desenlazan de nuevo\n",
        "random.shuffle(common_set)\n",
        "X[:], Y[:] = zip(*common_set)\n",
        "\n",
        "# Comprobación\n",
        "print('Tamaño del conjunto de entrenamiento (X): {}'.format(len(X)))\n",
        "print('Tamaño del conjunto de entrenamiento (Y): {}'.format(len(Y)))\n",
        "\n",
        "# Ejemplo de muestra\n",
        "pyplot.imshow(X[0])                         # Pintar por pantalla la imagen (X)\n",
        "print('Ejemplo de Y (0): {}'.format(Y[0]))  # Imprimir la palabra en texto (Y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aszh5Mter-I5"
      },
      "source": [
        "Vamos ahora a crear el modelo de red neuronal. Antes, necesitamos establecer las condiciones de las imágenes de entrada, ya que la red convolucional necesita unas dimensiones pre-establecidas para ser definida (¿seguro?).\n",
        "\n",
        "Con el objetivo de no distorsionar la letra, vamos a establecer un número de píxeles de altura fijos, y el ancho se hará acorde manteniendo la proporción."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMB3SIBpsZlV"
      },
      "source": [
        "# Función que rescala a una altura fija y mantiene la proporción ancho/alto\n",
        "def resize(image, height):\n",
        "    width = int(float(height * image.shape[1]) / image.shape[0])\n",
        "    sample_img = cv2.resize(image, (width, height))\n",
        "    return sample_img\n",
        "\n",
        "# Establecemos la altura a 40 píxeles\n",
        "img_height = 40\n",
        "\n",
        "# Realizamos el rescalado para todas las imagenes\n",
        "for idx,image in enumerate(X):\n",
        "  X[idx] = resize(image,img_height)\n",
        "\n",
        "# Comprobación\n",
        "pyplot.imshow(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWZl0tFGtIn_"
      },
      "source": [
        "Así pues, la red convolucional recibirá imagenes del alto establecido y el ancho máximo de entre todas las imágenes. Por lo tanto, necesitamos hacer *padding*: aquellas imagenes con un ancho real menor a este valor se rellenarán con valores nulos 0 en este caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0p2Z4iptKbG"
      },
      "source": [
        "# Calculamos la anchura máxima\n",
        "max_image_len = max([image.shape[1] for image in X])\n",
        "\n",
        "# Creamos un único paquete de datos\n",
        "encoder_input = np.zeros((len(X),img_height,max_image_len), dtype=np.float)\n",
        "\n",
        "# Encajamos la imagen en su posición correspondiente\n",
        "for idx, image in enumerate(X):\n",
        "    encoder_input[idx][:,:image.shape[1]] = image\n",
        "\n",
        "# Comprobación\n",
        "pyplot.imshow(encoder_input[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I25nxXHeNfXK"
      },
      "source": [
        "El paquete de imágenes todavía no esta preparado para ser usado por Keras, ya que ahora mismo cada imagen solo tiene alto y ancho. Así pues, necesitamos asignarle un número de canales, en este caso 1 al ser en escala de grises. Para ello, basta con expandir el número de dimensiones, sin modificar en absoluto los valores de las matrices.\n",
        "\n",
        "Además, sobre estas imagenes, vamos a utilizar un pequeño truco que ayuda a la convergencia del proceso de aprendizaje: las imágenes, en lugar de mostrar valores entre 0 y 255 (niveles de gris), van a estar normalizadas entre 0 y 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WA-TxlcNezt"
      },
      "source": [
        "# Normalizamos los valores de los píxeles\n",
        "encoder_input = encoder_input / 255.\n",
        "\n",
        "# Expandimos la última dimension (alto,ancho) -> (alto,ancho,1)\n",
        "encoder_input = np.expand_dims(encoder_input, axis=-1)\n",
        "\n",
        "# Comprobación\n",
        "print(encoder_input.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhOh8OovsdVg"
      },
      "source": [
        "Ahora es necesario preparar los datos de salida. En primer lugar, vamos a incluir los caracteres de inicio y final de frase, necesarios para el decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfaHmjmPuj_W"
      },
      "source": [
        "# Caracteres de comienzo y final\n",
        "output_sos = '<'\n",
        "output_eos = '>'\n",
        "\n",
        "# Se añaden a todos los nombres\n",
        "Y = [output_sos + name + output_eos for name in Y]\n",
        "\n",
        "# Comprobación\n",
        "print(Y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35rk-XXtubQf"
      },
      "source": [
        "Para la parte del decoder (las letras reconocidas), también necesitamos fijar la longitud máxima de las secuencias, ya que vamos a realizar un único paquete de datos. Como ya hemos comentado, las redes recurrentes tienen la ventaja de adaptarse a anchos variables; sin embargo, los modelos se entrenan usando tensores, que deben tener unas dimensiones fijas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bia-olLTsrxX"
      },
      "source": [
        "# Conjuntos de caracteres y conversores\n",
        "alphabet = set()\n",
        "\n",
        "for name in Y:\n",
        "    alphabet.update(list(name))\n",
        "\n",
        "alphabet_len = len(alphabet)\n",
        "\n",
        "print('Hay un total de ' + str(alphabet_len) + ' caracteres: ' + str(alphabet))\n",
        "\n",
        "alphabet_from_char_to_int = dict([(char, i) for i, char in enumerate(alphabet)])\n",
        "alphabet_from_int_to_char = dict([(i, char) for i, char in enumerate(alphabet)])\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of6YDbHVOzno"
      },
      "source": [
        "Creamos los paquetes de entrada y salida del decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDvAoPdHO6ny"
      },
      "source": [
        "max_output_len = max([len(name) for name in Y])\n",
        "\n",
        "decoder_input = np.zeros((len(Y),max_output_len,alphabet_len), dtype=np.float)\n",
        "decoder_output = np.zeros((len(Y),max_output_len,alphabet_len), dtype=np.float)\n",
        "\n",
        "for idx_s, output_sentence in enumerate(Y):\n",
        "    for idx_c, char in enumerate(output_sentence):\n",
        "        decoder_input[idx_s][idx_c][alphabet_from_char_to_int[char]] = 1.\n",
        "        if idx_c > 0:\n",
        "            decoder_output[idx_s][idx_c-1][alphabet_from_char_to_int[char]] = 1.\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb_RL1pjvCBd"
      },
      "source": [
        "Ya tenemos nuestro paquete de datos formado por tres vectores (entrada del encoder, entrada del decoder y salida del decoder)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEzdg70dvKzu"
      },
      "source": [
        "print ('Encoder input: ' + str(encoder_input.shape))\n",
        "print ('Decoder input: ' + str(decoder_input.shape))\n",
        "print ('Decoder output: ' + str(decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WLL6xjYvr5M"
      },
      "source": [
        "Definamos ahora una función que recibe los parámetros del problema y devuelve un modelo encoder (CNN) - decoder (RNN) ajustado a dichos parámetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWTOOMfrvqzr"
      },
      "source": [
        "from keras.layers import Input, Dense, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "def getmodel(img_height, img_width, alphabet_len):\n",
        "\n",
        "  # Comprobamos el formato de datos esperado\n",
        "  input_data = Input(name='input', shape=(img_height, img_width, 1))\n",
        "\n",
        "\n",
        "  # Encoder como CNN de tres capas\n",
        "  inner = Conv2D(32, (3, 3), padding='same', activation='relu')(input_data)\n",
        "  inner = MaxPooling2D(pool_size=(2,2))(inner)\n",
        "\n",
        "  inner = Conv2D(64, (3, 3), padding='same', activation='relu')(inner)\n",
        "  inner = MaxPooling2D(pool_size=(2,2))(inner)\n",
        "\n",
        "  inner = Conv2D(128, (3, 3), padding='same', activation='relu')(inner)\n",
        "  inner = MaxPooling2D(pool_size=(2,2))(inner)\n",
        "\n",
        "  # Conexion CNN con LSTM\n",
        "  x = Flatten()(inner)\n",
        "  h_0 = Dense(256, activation='tanh')(x)\n",
        "  c_0 = Dense(256, activation='tanh')(x)\n",
        "\n",
        "  initial_state = [h_0, c_0]\n",
        "\n",
        "  # Decoder como RNN\n",
        "  decoder_inputs = Input(shape=(None, alphabet_len))\n",
        "  decoder_outputs, _, _ = LSTM(256, return_sequences=True, return_state=True)(decoder_inputs, initial_state=initial_state)\n",
        "  decoder_dense = Dense(alphabet_len, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  # Creamos el modelo único\n",
        "  model = Model([input_data, decoder_inputs], decoder_outputs)\n",
        "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "# Extraemos los parámetros de los paquetes creados\n",
        "model = getmodel(encoder_input.shape[1],encoder_input.shape[2],alphabet_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD6epLfewr-K"
      },
      "source": [
        "El modelo está listo para ser entrenado y evaluado. Antes, vamos a realizar una partición de los datos disponibles para entrenamiento y validación: 99% y 1% respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygv0-sGswyYr"
      },
      "source": [
        "val_split = 0.01\n",
        "idx_split = int(len(X)*val_split)\n",
        "\n",
        "# Conjunto de entrenamiento: paquetes codificados\n",
        "x_train = encoder_input[idx_split:]\n",
        "y_train = decoder_input[idx_split:]\n",
        "t_train = decoder_output[idx_split:]\n",
        "\n",
        "# Conjunto de validación: paquetes codificados + salida esperada\n",
        "x_val = encoder_input[:idx_split]\n",
        "y_val = decoder_input[:idx_split]\n",
        "t_val = decoder_output[:idx_split]\n",
        "i_val = Y[:idx_split]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9if7TfDXw5fq"
      },
      "source": [
        "Entrenamos durante 15 épocas, monitorizando cuantitativamente por época (loss) y cualitativamente cada 3 épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egnkkf7dw8LS"
      },
      "source": [
        "# Entrenamos durante 5 super-epocas\n",
        "for super_epoch in range(5):\n",
        "    print('Super-epoca: ' + str(super_epoch))\n",
        "\n",
        "    # 3 épocas en cada iteración\n",
        "    model.fit([x_train, y_train], t_train, verbose=1, batch_size=16, epochs=3, validation_data=[[x_val,y_val], t_val])\n",
        "\n",
        "    # Predecimos sobre el conjunto de validación\n",
        "    batch_prediction = model.predict([x_val,y_val],batch_size=16)\n",
        "\n",
        "    # Comprobamos las 5 primeras imágenes\n",
        "    for idx,sentence_prediction in enumerate(batch_prediction[:5]):\n",
        "\n",
        "        raw_predicted_sequence = [alphabet_from_int_to_char[char] for char in np.argmax(sentence_prediction,axis=1)]\n",
        "\n",
        "        predicted_sentence = output_sos\n",
        "        for char in raw_predicted_sequence:\n",
        "            predicted_sentence += char\n",
        "            if char == output_eos:\n",
        "                break\n",
        "\n",
        "        print( 'Predicción\\t: ' + str(predicted_sentence) )\n",
        "        print( 'Sal. esperada\\t: ' + str(i_val[idx]) )\n",
        "        print()\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRqtASp1cjeD"
      },
      "source": [
        "Muy lentamente, el modelo va aprendiendo a reconocer los caracteres de las imágenes.\n",
        "\n",
        "### Consideraciones\n",
        "\n",
        "Los resultados obtenidos en este proyecto seguramente están lejos de la fiabilidad que alcanzan los sistemas actuales. No obstante, la formulación de los modelos del estado del arte son similares: la diferencia rádica especialmente en el tamaño del modelo de red y la cantidad de datos para entrenar. En este ejemplo es sencillo aumentar tanto el tamaño de la red como el tamaño del conjunto de entrenamiento.\n",
        "\n",
        "También existen otras cuestiones que quizá hayas obviado en este ejercicio como la búsqueda de hiper-parámetros adecuados, funciones generadoras para reducir el *padding* o modelos de atención (ver final de Seccion 3.2.1)."
      ]
    }
  ]
}