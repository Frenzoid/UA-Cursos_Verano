{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5cNF2JaztC9"
      },
      "source": [
        "# Sesión 4.1. Teoría - Redes Neuronales Recurrentes Avanzadas\n",
        "Curso 2022-23\n",
        "\n",
        "Profesor: [Jorge Calvo Zaragoza](mailto:jcalvo@dlsi.ua.es)\n",
        "\n",
        "## Resumen\n",
        "En esta sesión:\n",
        "  * Profundizaremos en el uso de redes recurrentes en Keras.\n",
        "  * Introduciremos el encaje vectorial de palabras (*word embeddings*)\n",
        "  * Explicaremos mecanismos adicionales que facilitan el uso de RNN en Keras (funciones generadoras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOYZKQalCU7c"
      },
      "source": [
        "## Entradas discretas\n",
        "\n",
        "El *one-hot encoding* es una representación numérica de variables discretas. Consiste en asumir vectores de características de dimension igual al número de diferentes valores del conjunto discreto de la variable a codificar. Cada dimension corresponde a un único elemento de dicho conjunto. Hasta ahora las hemos usado para la salida de la red, pero **se puede usar también para la entrada**.\n",
        "\n",
        "\n",
        "Normalmente las redes recurrentes se usan para tareas de *procesamiento de lenguaje natural*. En este tipo de problemas, los elementos de entrada son  elementos discretos como caracteres o palabras, y no características numéricas. Si una red neuronal sólo *entiende* de números, ¿cómo podemos indicarle este tipo de características en la entrada? Del mismo modo que se indica para las categorías: utilizando una codificación one-hot.\n",
        "\n",
        "El one-hot encoding tiene una particularidad importante: no asume ninguna relación entre los elementos del vocabulario. Dicho de otro modo, todos los elementos del conjunto son equidistantes entre sí en la codificación one-hot (distancia hamming 2). Por otra parte, el one-hot encoding tiene la máxima redundancia; en realidad, para codificar $N$ elementos, tan sólo harían falta vectores de $log(N)$ bits.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lin9D6FkCi1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc19e3f-59df-4a9d-ccce-37901dc05577"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "Asumamos el principio de un ejercicio en el cual\n",
        "la red va a predecir sobre secuencias que indican\n",
        "colores.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Definimos el vocabulario de colores\n",
        "vocabulary = {'rojo', 'amarillo', 'azul', 'verde', 'lila' ,'naranja'}\n",
        "\n",
        "# Asignamos a cada palabra un índice numérico\n",
        "word_to_int = dict([(char, i) for i, char in enumerate(vocabulary)])\n",
        "print('Asignacion de indice a palabras')\n",
        "print(word_to_int)\n",
        "\n",
        "# De esta forma las frases se componen de secuencias de vectores de 6 elementos\n",
        "sentence = 'rojo amarillo naranja'\n",
        "tokenized_sentence = sentence.split()\n",
        "encoded_sentence = np.zeros([len(tokenized_sentence),len(vocabulary)])\n",
        "\n",
        "# Para cada palabra, activamos la posición que corresponde a su identificador\n",
        "for i,c in enumerate(sentence.split()):\n",
        "  encoded_sentence[i][ word_to_int[c] ] = 1\n",
        "\n",
        "# Comprobación\n",
        "print()\n",
        "print('Frase original: {}'.format(sentence))\n",
        "\n",
        "print()\n",
        "print('Frase secuencial:')\n",
        "print(str(tokenized_sentence))\n",
        "\n",
        "print()\n",
        "print('Frase codificada:')\n",
        "print(str(encoded_sentence))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asignacion de indice a palabras\n",
            "{'azul': 0, 'naranja': 1, 'verde': 2, 'amarillo': 3, 'lila': 4, 'rojo': 5}\n",
            "\n",
            "Frase original: rojo amarillo naranja\n",
            "\n",
            "Frase secuencial:\n",
            "['rojo', 'amarillo', 'naranja']\n",
            "\n",
            "Frase codificada:\n",
            "[[0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoFMNIY4DNYq"
      },
      "source": [
        "\n",
        "#### Estado actual\n",
        "\n",
        "A menudo encontramos tareas para las cuales el vocabulario a tener en cuenta es inmenso. Por ejemplo, el idioma español tiene alrededor de 100.000 vocablos distintos. Además, ¿qué ocurre con las palabras muy poco frecuentes o que incluso están fuera del vocabulario que habíamos planeado inicialmente?\n",
        "\n",
        "Una posibilidad para lidiar con este escenario es bajar a nivel de caracteres, cuyo vocabulario suele ser mucho más restringido. El problema es que la red neuronal tiene que hacer un esfuerzo mayor en inferir las relaciones entre los distintos elementos de la entrada. En la actualidad, es común utilizar un enfoque intermedio basado en sub-palabras, con criterios tomados de teoría de la información."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4XjB5dqMTz8"
      },
      "source": [
        "## Encaje de palabras (Word Embedding)\n",
        "\n",
        "El one-hot encoding permite al usuario proporcionar a la red una entrada en la cual cada elemento tiene una representación equidistante con respecto a cualquier otro. Obviamente, para una tarea específica, esta asunción no es valida (ni útil). Por ejemplo, en problmas de *procesamiento de lenguaje natural* hay palabras que tienen un rol o un significado similar y por tanto, sería adecuado que tuvieran una representación interna similar.\n",
        "\n",
        "Idealmente, queremos representar cada palabra mediante un vector numérico en el cual las palabras similares (en rol o en significado) esten cerca en ese espacio. Esto se conoce como *embedding*.  Siguiendo los principios del deep learning, queremos que la red aprenda estas relaciones por sí sola en lugar de establecer el embedding siguiendo reglas heurísticas o basadas en nuestra propia intuición.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bHTDUtpMfHB"
      },
      "source": [
        "\n",
        "\n",
        "### word2vec\n",
        "\n",
        "La idea de realizar realizar embeddings es muy antigua. Sin embargo, su popularidad se incrementó considerablemente a partir del surgimiento de los modelos *word2vec*.\n",
        "\n",
        "\n",
        "#### Idea\n",
        "\n",
        "\n",
        "![texto alternativo](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/img/Bottou-WordSetup.png)\n",
        "\n",
        "#### Análisis\n",
        "\n",
        "![texto alternativo](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/img/Colbert-WordTable2.png)\n",
        "\n",
        "#### Representación semántica\n",
        "\n",
        "[Visualizacion](http://metaoptimize.s3.amazonaws.com/cw-embeddings-ACL2010/embeddings-mostcommon.EMBEDDING_SIZE=50.png)\n",
        "\n",
        "![texto alternativo](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/img/Mikolov-GenderVecs.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atUyx-GvMtVX"
      },
      "source": [
        "### Capas de embedding en Keras\n",
        "\n",
        "En Keras, la capa de embedding no se hace a través de operaciones matemáticas. En su lugar, lo que se hace es acceder a una tabla look-up que asocia cada entero con una posición. La capa Embedding de Keras recibe directamente el entero que identifica un elemento del vocabulario (sin necesidad de hacer un one-hot encoding) y devuelve su representación densa. Los parámetros de la capa son:\n",
        "* **input_dim**: tamaño del vocabulario de entrada (por ejemplo, número de palabras del problema)\n",
        "* **output_dim**: tamaño del espacio vectorial donde se encaja la entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHoGm3wxMyWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b651a6-44f4-49d7-e5b2-cfe29383b2ae"
      },
      "source": [
        "%%capture --no-stdout\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Definimos el tamaño del vocabulario y las dimensiones del espacio latente\n",
        "tam_vocabulario = 5\n",
        "tam_embedding   = 8\n",
        "\n",
        "# Definimos una entrada de longitud variable\n",
        "capa_entrada = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "\n",
        "# Añadimos la capa de embedding\n",
        "embedding = tf.keras.layers.Embedding(input_dim=tam_vocabulario,\n",
        "                      output_dim=tam_embedding)(capa_entrada)\n",
        "\n",
        "# Creamos un modelo a partir de estas dos capas\n",
        "model = tf.keras.models.Model(capa_entrada,embedding)\n",
        "model.summary()\n",
        "\n",
        "# Probamos a 'predecir' a través de esta red\n",
        "codificacion_entera = [4,1,3,3,3]\n",
        "codificacion_embedding = model.predict(np.asarray([codificacion_entera]))\n",
        "\n",
        "print()\n",
        "print('Representación de {}'.format( str(codificacion_entera) ))\n",
        "print(codificacion_embedding)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 8)           40        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40\n",
            "Trainable params: 40\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "\n",
            "Representación de [4, 1, 3, 3, 3]\n",
            "[[[ 0.04419135  0.03557061  0.00836201 -0.03705425  0.01063796\n",
            "    0.02537375  0.01366648  0.00534002]\n",
            "  [-0.02562686 -0.01458812  0.02063059 -0.01800523 -0.04325141\n",
            "   -0.03176136  0.01700329  0.02383772]\n",
            "  [ 0.00808215  0.03995105  0.00221574  0.03086218  0.00888181\n",
            "    0.00838411  0.01129188 -0.01288737]\n",
            "  [ 0.00808215  0.03995105  0.00221574  0.03086218  0.00888181\n",
            "    0.00838411  0.01129188 -0.01288737]\n",
            "  [ 0.00808215  0.03995105  0.00221574  0.03086218  0.00888181\n",
            "    0.00838411  0.01129188 -0.01288737]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-BN56hEMlz4"
      },
      "source": [
        "A pesar de esta implementación, los valores de esta tabla se aprenden de manera específica para la tarea en cuestión con los medios convencionales de entrenamiento de redes neuronales.\n",
        "\n",
        "A menudo es interesante importar los embeddings obtenidos mediante un word2vec de un dominio similar para utilizarlo como punto de partida en nuestra tarea.\n",
        "\n",
        "Keras facilita este escenario permitiendo establecer los pesos iniciales de la capa mediante el parámetro *weights*. Además, nos permite especificar si queremos modificar los pesos en nuestro propio entrenamiento o no.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t40QRDv2M2ob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3003e858-00b1-408b-cea3-f6a2d3f26eb0"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding\n",
        "import numpy as np\n",
        "\n",
        "# Definimos el tamaño del vocabulario y las dimensiones del espacio latente\n",
        "tam_vocabulario = 5\n",
        "tam_embedding   = 3\n",
        "\n",
        "# Creamos una matriz de embedding aleatoria\n",
        "matriz_embedding = np.random.rand(tam_vocabulario,tam_embedding)\n",
        "\n",
        "print('Tabla de embedding inicial')\n",
        "for idx, representation in enumerate(matriz_embedding):\n",
        "  print(idx,representation)\n",
        "\n",
        "# Definimos una entrada de longitud variable\n",
        "capa_entrada = tf.keras.layers.Input(shape=(None,), dtype='int32')\n",
        "\n",
        "# Proporcionamos la matriz anterior como parámetro\n",
        "embedding = tf.keras.layers.Embedding(input_dim=tam_vocabulario,\n",
        "                      output_dim=tam_embedding,\n",
        "                      weights=[matriz_embedding],\n",
        "                      trainable=True)(capa_entrada)\n",
        "\n",
        "# Creamos el modelo de embedding\n",
        "model = tf.keras.models.Model(capa_entrada,embedding)\n",
        "\n",
        "# Comprobamos el embedding sobre este modelo\n",
        "codificacion_entera = [4,1,3,3,3]\n",
        "codificacion_embedding = model.predict(np.asarray([codificacion_entera]))\n",
        "\n",
        "print()\n",
        "print('Representación de {}'.format( str(codificacion_entera) ))\n",
        "print(codificacion_embedding)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla de embedding inicial\n",
            "0 [0.58007216 0.26285934 0.64265632]\n",
            "1 [0.71907505 0.20183993 0.14741692]\n",
            "2 [0.35635058 0.47093649 0.16693704]\n",
            "3 [0.73443321 0.98123054 0.09198138]\n",
            "4 [0.16951729 0.21098193 0.34760449]\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "\n",
            "Representación de [4, 1, 3, 3, 3]\n",
            "[[[0.1695173  0.21098194 0.34760448]\n",
            "  [0.719075   0.20183994 0.14741692]\n",
            "  [0.73443323 0.98123056 0.09198138]\n",
            "  [0.73443323 0.98123056 0.09198138]\n",
            "  [0.73443323 0.98123056 0.09198138]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOeyW5tUUvOE"
      },
      "source": [
        "### Funciones generadoras\n",
        "\n",
        "En el caso de las tareas resueltas con RNN es habitual tener secuencias de longitud variable. Aunque, por definición, las RNN pueden manejarlas, internamente Keras trabaja con lotes de datos de dimensión fija para un procesamiento más eficiente.\n",
        "\n",
        "Para aliviar este problema, hay dos alternativas: crear un lote por cada secuencia (lotes de 1 muestra), lo cual es ineficiente, o usar la técnica de *padding*. El *padding* (relleno) consiste en calcular la secuencia más larga y establecer las dimensiones de acuerdo con este valor. Las secuencias más cortas se rellenan con valores nulos (normalmente 0).\n",
        "\n",
        "El problema es que, cuando las diferencias entre las longitudes son muy altas, el efecto del *padding* es muy severo y causa un aprendizaje menos efectivo. Sin embargo, hay una solución intermedia, elegante, para hacer frente a este problema: los mini-lotes (*mini batches*). Es decir, construir pequeños lotes y aplicar *padding* en cada mini-lote de forma independiente.\n",
        "\n",
        "Para poder hacer esto en Keras, es necesario usar funciones *generadoras*. Las *generadoras* son funciones de Python que preparan tales lotes. En cada llamada, el generador prepara el siguiente lote para ser considerado y se congela hasta que se vuelve a llamar para generar el siguiente lote. Las funciones del generador nos permiten devolver lotes que contienen un número fijo de elementos. Luego podemos hacer el relleno a nivel de lote (*intra-batch padding*), reduciendo así el problema mencionado anteriormente.\n",
        "\n",
        "El siguiente código proporciona un ejemplo intuitivo de este comportamiento (prestad especial atención a las palabras reservadas **yield** y **next**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKyZ5L0hUvyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0c17a8-8ff5-487d-87ee-8fd35b615f23"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Conjunto de datos de muestras de longitud variable\n",
        "muestra = [[1,2], [3,4,5], [6,7,8,9], [10]]\n",
        "\n",
        "# Funcion generadora\n",
        "def ejemplo_generador(data, tam_lote):\n",
        "\n",
        "  while True:\n",
        "      # Creamos lotes de tamaño `tam_lote`\n",
        "      for idx in range(0,len(data),tam_lote):\n",
        "\n",
        "        # Datos de este lote\n",
        "        lote = data[idx:idx+tam_lote] # data[0:2) -> 0,1, data[2:4) -> 2,3\n",
        "\n",
        "        # Calculamos longitud maxima\n",
        "        longitud_maxima = max([len(b) for b in lote])\n",
        "\n",
        "        # Creamos un lote de dimensiones fijas y relleno de 0\n",
        "        lote_dimension_fija = np.zeros((len(lote),longitud_maxima),\n",
        "                                       dtype=int)\n",
        "\n",
        "\n",
        "        # Ponemos los datos reales en el lote secuencia a secuencia\n",
        "        for idx_s, secuencia in enumerate(lote):\n",
        "          # Y dato a dato\n",
        "          for idx_d, dato in enumerate(secuencia):\n",
        "            lote_dimension_fija[idx_s,idx_d] = dato\n",
        "\n",
        "        yield lote_dimension_fija\n",
        "\n",
        "\n",
        "# Inspeccionamos la muestra\n",
        "print('Muestra')\n",
        "print(muestra)\n",
        "\n",
        "\n",
        "# Creamos el generador\n",
        "generador = ejemplo_generador(muestra, tam_lote = 2)\n",
        "\n",
        "# Inspeccionamos la salida del generador\n",
        "print()\n",
        "print('Lote',1)\n",
        "x  = next(generador)\n",
        "print(x)\n",
        "print('Forma {}'.format( x.shape ))\n",
        "print()\n",
        "print('Lote',2)\n",
        "x  = next(generador)\n",
        "print(x)\n",
        "print('Forma {}'.format( x.shape ))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Muestra\n",
            "[[1, 2], [3, 4, 5], [6, 7, 8, 9], [10]]\n",
            "\n",
            "Lote 1\n",
            "[[1 2 0]\n",
            " [3 4 5]]\n",
            "Forma (2, 3)\n",
            "\n",
            "Lote 2\n",
            "[[ 6  7  8  9]\n",
            " [10  0  0  0]]\n",
            "Forma (2, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2araI0YWSgB"
      },
      "source": [
        "#### Bucketting\n",
        "\n",
        "Otro concepto relacionado es *bucketting*, que complementa al *padding* para hacerlo más adecuado al entrenar una red neuronal. Esta estrategia primero ordena el conjunto de datos para que los elementos dentro del mismo lote tengan una longitud similar. Esto minimiza el *padding* intra-lote, por lo que se acerca a la forma óptima (pero ineficiente) de realizar el proceso de entrenamiento secuencia a secuencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XohCvuxWS-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b46767-9455-44be-8a07-4ba83f435646"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Conjunto de datos de muestras de longitud variable\n",
        "muestra = [[1,2], [3,4,5], [6,7,8,9], [10]]\n",
        "\n",
        "# Funcion generadora que implementa bucketting\n",
        "def bucketting_generador(data, tam_lote):\n",
        "\n",
        "    data.sort(key = len) # [!] Ordenamos los datos primero\n",
        "\n",
        "    while True: # Infinita -> sirve datos continuamente\n",
        "      # Aqui podriamos re-barajar los datos\n",
        "\n",
        "      # Creamos lotes de tamaño `tam_lote`\n",
        "      for idx in range(0,len(data),tam_lote):\n",
        "\n",
        "        # Datos de este lote\n",
        "        lote = data[idx:idx+tam_lote]\n",
        "\n",
        "        # Calculamos longitud maxima\n",
        "        longitud_maxima = max([len(b) for b in lote])\n",
        "\n",
        "        # Creamos un lote de dimensiones fijas y relleno de 0\n",
        "        lote_dimension_fija = np.zeros((len(lote),longitud_maxima),\n",
        "                                       dtype=int)\n",
        "\n",
        "\n",
        "        # Ponemos los datos reales en el lote secuencia a secuencia\n",
        "        for idx_s, secuencia in enumerate(lote):\n",
        "          # Y dato a dato\n",
        "          for idx_d, dato in enumerate(secuencia):\n",
        "            lote_dimension_fija[idx_s,idx_d] = dato\n",
        "\n",
        "        yield lote_dimension_fija\n",
        "\n",
        "\n",
        "# Inspeccionamos la muestra\n",
        "print('Muestra')\n",
        "print(muestra)\n",
        "\n",
        "# Creamos el generador\n",
        "generador = bucketting_generador(muestra, tam_lote = 2)\n",
        "\n",
        "# Inspeccionamos la salida del generador\n",
        "print()\n",
        "print('Lote',1)\n",
        "x  = next(generador)\n",
        "print(x)\n",
        "print('Forma {}'.format( x.shape ))\n",
        "print()\n",
        "print('Lote',2)\n",
        "x  = next(generador)\n",
        "print(x)\n",
        "print('Forma {}'.format( x.shape ))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Muestra\n",
            "[[1, 2], [3, 4, 5], [6, 7, 8, 9], [10]]\n",
            "\n",
            "Lote 1\n",
            "[[10  0]\n",
            " [ 1  2]]\n",
            "Forma (2, 2)\n",
            "\n",
            "Lote 2\n",
            "[[3 4 5 0]\n",
            " [6 7 8 9]]\n",
            "Forma (2, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FoHHBe6WUF9"
      },
      "source": [
        "#### Keras y funciones generadoras\n",
        "\n",
        "Keras permite integrar de forma natural las funciones generadoras en el proceso de entrenamiento mediante la función **fit** (en versiones anteriores  de Keras se llamaba a *fit_generator*). Esta función acepta una función generadora y va cargando los mini-lotes según se van generando. En este caso, los parámetros de la función **fit** son diferentes a los convencionales (como veremos más adelante)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALr_RE8bWp7y"
      },
      "source": [
        "## Ejemplo de RNN con funciones generadoras\n",
        "\n",
        "En este ejemplo vamos a entrenar una red que detecte si una palabra ha sido generada aleatoriamente o proviene de un vocabulario. Para ello:\n",
        "\n",
        "* Vamos a descargar un vocabulario de palabras de la web.\n",
        "* Vamos a asignar un número a cada caracter.\n",
        "* Vamos a implementar una función generadora que nos sirva lotes de palabras de la misma longitud.\n",
        "  * Tanto palabras del vocabulario como generadas aleatoriamente.\n",
        "* Vamos a definir el modelo de red RNN.\n",
        "* Vamos a entrenar la red y evaluarlo interactivamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2qyzO17WrPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8bb225-4ee0-4c47-d849-4cffef719496"
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "# Funcion que nos devuelve palabras de un vocabualrio\n",
        "def vocabulario_palabras():\n",
        "  url_palabras = \"https://svnweb.freebsd.org/csrg/share/dict/words?view=co&content-type=text/plain\"\n",
        "  palabras = urllib.request.urlopen(urllib.request.Request(url_palabras, headers={'User-Agent': 'Mozilla/5.0'})).read().decode().splitlines()\n",
        "\n",
        "  # Filtramos las palabras que solo tengan caracteres alfabeticos y estén en minúsculas\n",
        "  palabras_seleccionadas  = [palabra for palabra in palabras\n",
        "                             if re.match('^[a-zA-Z]+$',palabra) and palabra.islower()]\n",
        "\n",
        "  return palabras_seleccionadas\n",
        "\n",
        "\n",
        "# Definimos el vocabulario a partir de la función anterior\n",
        "vocabulario = vocabulario_palabras()\n",
        "print()\n",
        "print('No. palabras: {}'.format(len(vocabulario)))\n",
        "\n",
        "# Nuestras categorías son las minusculas + símbolo de padding\n",
        "conjunto_caracteres = 'P' + string.ascii_lowercase\n",
        "tam_conjunto_caracteres = len(conjunto_caracteres)\n",
        "\n",
        "print()\n",
        "print('No. de caracteres: {} ({})'.format(tam_conjunto_caracteres,conjunto_caracteres))\n",
        "\n",
        "# Creamos los conversores de char a int y viceversa\n",
        "char_to_int = dict([(char, i) for i, char in enumerate(conjunto_caracteres)])\n",
        "int_to_char = dict([(i, char) for i, char in enumerate(conjunto_caracteres)])\n",
        "\n",
        "print()\n",
        "print('Char to int: {}'.format(char_to_int))\n",
        "print('Int to char: {}'.format(int_to_char))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No. palabras: 20409\n",
            "\n",
            "No. de caracteres: 27 (Pabcdefghijklmnopqrstuvwxyz)\n",
            "\n",
            "Char to int: {'P': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "Int to char: {0: 'P', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuDOSXzmWrxD"
      },
      "source": [
        "Definimos la función generadora de lotes para entrenar la red:\n",
        "* Asumimos que las variables anteriores son accesibles (ahorramos parámetros)\n",
        "* Tendremos dos tipos de palabras: inventadas (0) y reales (1)\n",
        "* Las palabras inventadas se generarán en esta misma función\n",
        "* Las palabras reales se tomarán del vocabulario descargado y filtrado.\n",
        "\n",
        "Después vamos a inspeccionar la función generadora para asegurarnos de que hace lo que se espera."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KyTeNN6W4eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a3d448-d708-4760-e7b4-0c52ad9142e4"
      },
      "source": [
        "\n",
        "def generador(tam_lote = 32):\n",
        "  # Bucle principal de la generadora\n",
        "  while True:\n",
        "    X  = []               # Variable donde guardar las palabras\n",
        "    Y  = []               # Variable donde guardar las etiquetas\n",
        "\n",
        "    # Cada lote tiene 'tam_lote' palabras\n",
        "    for _ in range(tam_lote):\n",
        "\n",
        "      if random.random() < 0.5:     # Al 50 % tomamos una real o una inventada\n",
        "\n",
        "        # Generamos una cadena aleatoria de tamaño 3-8 (sin padding!)\n",
        "        palabra = ''.join(random.choice(string.ascii_lowercase)\n",
        "                          for i in range(random.randint(3,8)))\n",
        "        etiqueta = False\n",
        "\n",
        "      else:\n",
        "        # Usar real\n",
        "        palabra = random.choice(vocabulario)\n",
        "        etiqueta = True\n",
        "\n",
        "      X.append(palabra)\n",
        "      Y.append(etiqueta)\n",
        "\n",
        "    # Preparamos el lote\n",
        "    max_longitud = max([len(palabra) for palabra in X])\n",
        "\n",
        "    X_lote = np.zeros((tam_lote, max_longitud), dtype=np.int32)\n",
        "    Y_lote = np.zeros((tam_lote, 1), dtype=np.int32)\n",
        "\n",
        "    for idx_p, palabra in enumerate(X):\n",
        "      for idx_c, caracter in enumerate(palabra):\n",
        "          X_lote[idx_p][idx_c] = char_to_int[caracter] # 'a' -> 1, 'b' -> 2, etc.\n",
        "\n",
        "    for idx_e, etiqueta in enumerate(Y):\n",
        "      Y_lote[idx_e] = 1 if etiqueta == True else 0\n",
        "\n",
        "\n",
        "    yield X_lote, Y_lote\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Prueba de la función generadora\n",
        "\n",
        "g = generador(2)\n",
        "X_lote, Y_lote = next(g)\n",
        "print(X_lote[0])\n",
        "print(Y_lote[0])\n",
        "\n",
        "print('Forma del lote X: {}'.format(X_lote.shape))\n",
        "print('Forma del lote Y: {}'.format(Y_lote.shape))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9 14 22  5 18 20  9  2 12  5]\n",
            "[1]\n",
            "Forma del lote X: (2, 10)\n",
            "Forma del lote Y: (2, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykjadyvBW5rm"
      },
      "source": [
        "A continuación vamos a construir la RNN y a entrenarla. Para aprender este problema de secuencias vamos a crear una red con dos capas:\n",
        "\n",
        "* La primera capa estará formada por 16 neuronas LSTM.\n",
        "* La segunda capa será de tipo *Dense* y tendrá una única neurona (suficiente para problemas de clasificación binaria)\n",
        "* La activación de la segunda capa será de tipo *Sigmoid*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUOUMtq_W7Zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ff142c-46e4-483f-8641-8a71d4213a9e"
      },
      "source": [
        "%%capture --no-stdout\n",
        "\n",
        "# ------------------------------------\n",
        "# Parámetros del problema\n",
        "\n",
        "TAM_LOTES = 32\n",
        "NUM_CARACTERISTICAS = tam_conjunto_caracteres\n",
        "\n",
        "# ------------------------------------\n",
        "# Modelo de red\n",
        "print()\n",
        "print('Construimos el modelo de red...')\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.InputLayer((None,))) # `None`significa que no es fijo !\n",
        "model.add(tf.keras.layers.Embedding(input_dim=tam_conjunto_caracteres, output_dim=4))\n",
        "model.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
        "model.add(tf.keras.layers.LSTM(16))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
        "model.summary()\n",
        "\n",
        "# ------------------------------------\n",
        "# Entrenamiento\n",
        "print()\n",
        "print('Entrenamos la red...')\n",
        "\n",
        "gen_training = generador(TAM_LOTES)\n",
        "gen_validacion = generador(TAM_LOTES)\n",
        "\n",
        "# La funcion `fit` tiene particularidades...\n",
        "history = model.fit(gen_training,\n",
        "                    steps_per_epoch=32,\n",
        "                    validation_data=gen_validacion,\n",
        "                    validation_steps=32,\n",
        "                    epochs=30,\n",
        "                    verbose=2)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Construimos el modelo de red...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 4)           108       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 32)          4736      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 16)                3136      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,997\n",
            "Trainable params: 7,997\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Entrenamos la red...\n",
            "Epoch 1/30\n",
            "32/32 - 7s - loss: 0.6924 - val_loss: 0.6890 - 7s/epoch - 206ms/step\n",
            "Epoch 2/30\n",
            "32/32 - 1s - loss: 0.6571 - val_loss: 0.5609 - 889ms/epoch - 28ms/step\n",
            "Epoch 3/30\n",
            "32/32 - 1s - loss: 0.4803 - val_loss: 0.4265 - 904ms/epoch - 28ms/step\n",
            "Epoch 4/30\n",
            "32/32 - 1s - loss: 0.4354 - val_loss: 0.3753 - 751ms/epoch - 23ms/step\n",
            "Epoch 5/30\n",
            "32/32 - 1s - loss: 0.3967 - val_loss: 0.4167 - 598ms/epoch - 19ms/step\n",
            "Epoch 6/30\n",
            "32/32 - 1s - loss: 0.3560 - val_loss: 0.3708 - 599ms/epoch - 19ms/step\n",
            "Epoch 7/30\n",
            "32/32 - 1s - loss: 0.3847 - val_loss: 0.3777 - 587ms/epoch - 18ms/step\n",
            "Epoch 8/30\n",
            "32/32 - 1s - loss: 0.3554 - val_loss: 0.3700 - 606ms/epoch - 19ms/step\n",
            "Epoch 9/30\n",
            "32/32 - 1s - loss: 0.3207 - val_loss: 0.3790 - 808ms/epoch - 25ms/step\n",
            "Epoch 10/30\n",
            "32/32 - 1s - loss: 0.3274 - val_loss: 0.3186 - 764ms/epoch - 24ms/step\n",
            "Epoch 11/30\n",
            "32/32 - 1s - loss: 0.3318 - val_loss: 0.3350 - 608ms/epoch - 19ms/step\n",
            "Epoch 12/30\n",
            "32/32 - 1s - loss: 0.3197 - val_loss: 0.3046 - 769ms/epoch - 24ms/step\n",
            "Epoch 13/30\n",
            "32/32 - 1s - loss: 0.3166 - val_loss: 0.2960 - 747ms/epoch - 23ms/step\n",
            "Epoch 14/30\n",
            "32/32 - 1s - loss: 0.3263 - val_loss: 0.3109 - 622ms/epoch - 19ms/step\n",
            "Epoch 15/30\n",
            "32/32 - 1s - loss: 0.3219 - val_loss: 0.2927 - 632ms/epoch - 20ms/step\n",
            "Epoch 16/30\n",
            "32/32 - 1s - loss: 0.3175 - val_loss: 0.2713 - 761ms/epoch - 24ms/step\n",
            "Epoch 17/30\n",
            "32/32 - 1s - loss: 0.3083 - val_loss: 0.2681 - 633ms/epoch - 20ms/step\n",
            "Epoch 18/30\n",
            "32/32 - 1s - loss: 0.2924 - val_loss: 0.2749 - 794ms/epoch - 25ms/step\n",
            "Epoch 19/30\n",
            "32/32 - 1s - loss: 0.2611 - val_loss: 0.2811 - 907ms/epoch - 28ms/step\n",
            "Epoch 20/30\n",
            "32/32 - 1s - loss: 0.2984 - val_loss: 0.2921 - 992ms/epoch - 31ms/step\n",
            "Epoch 21/30\n",
            "32/32 - 1s - loss: 0.2987 - val_loss: 0.3009 - 875ms/epoch - 27ms/step\n",
            "Epoch 22/30\n",
            "32/32 - 1s - loss: 0.3014 - val_loss: 0.3068 - 763ms/epoch - 24ms/step\n",
            "Epoch 23/30\n",
            "32/32 - 1s - loss: 0.2990 - val_loss: 0.3260 - 618ms/epoch - 19ms/step\n",
            "Epoch 24/30\n",
            "32/32 - 1s - loss: 0.2790 - val_loss: 0.2535 - 786ms/epoch - 25ms/step\n",
            "Epoch 25/30\n",
            "32/32 - 1s - loss: 0.2691 - val_loss: 0.3327 - 592ms/epoch - 18ms/step\n",
            "Epoch 26/30\n",
            "32/32 - 1s - loss: 0.2609 - val_loss: 0.2857 - 616ms/epoch - 19ms/step\n",
            "Epoch 27/30\n",
            "32/32 - 1s - loss: 0.2943 - val_loss: 0.2746 - 570ms/epoch - 18ms/step\n",
            "Epoch 28/30\n",
            "32/32 - 1s - loss: 0.2778 - val_loss: 0.3044 - 605ms/epoch - 19ms/step\n",
            "Epoch 29/30\n",
            "32/32 - 1s - loss: 0.2875 - val_loss: 0.3297 - 772ms/epoch - 24ms/step\n",
            "Epoch 30/30\n",
            "32/32 - 1s - loss: 0.2793 - val_loss: 0.2967 - 791ms/epoch - 25ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1q6qwdRW9cA"
      },
      "source": [
        "Por último vamos a evaluar el modelo de red con los pesos aprendidos. Para esto usaremos nuevas secuencias y comprobaremos si la predicción es correcta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RTqe2UpW9sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd51084d-b8eb-447e-f2b8-2443afe72a47"
      },
      "source": [
        "%%capture --no-stdout\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Función que evalua una palabra\n",
        "# - Asumimos que todas las variables anteriores son accesibles\n",
        "# - Convertimos la palabra de entrada en un lote unitario\n",
        "# - Predecimos utilizando la red\n",
        "\n",
        "def evaluate(palabra):\n",
        "  x = np.zeros((1, len(palabra)),\n",
        "               dtype=np.int32)\n",
        "\n",
        "  for idx_c, caracter in enumerate(palabra):\n",
        "    x[0][idx_c] = char_to_int[caracter]\n",
        "\n",
        "\n",
        "  prediccion = model.predict(x, verbose=0)[0]\n",
        "\n",
        "  print('Nivel de `realismo` de {}: {}'.format(palabra,prediccion))\n",
        "\n",
        "\n",
        "# Vamos a evaluar la siguiente secuencias\n",
        "evaluate(\"d\")\n",
        "evaluate(\"housing\")\n",
        "evaluate(\"adfjkljk\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nivel de `realismo` de d: [0.43565106]\n",
            "Nivel de `realismo` de housing: [0.8638271]\n",
            "Nivel de `realismo` de adfjkljk: [0.02914312]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMGLGfieLR9C"
      },
      "source": [
        "&nbsp;\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        " Vamos a practicar &#10158; [Ejercicio de predicción de opinion de una película](https://colab.research.google.com/drive/1r2P6ySoSRe36YJbO2kFtY7sxvOdvoRLj?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    }
  ]
}