{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5cNF2JaztC9"
      },
      "source": [
        "# Sesión 4.2. Teoría - Redes Neuronales Recurrentes Avanzadas\n",
        "\n",
        "Profesor: [Jorge Calvo Zaragoza](mailto:jcalvo@dlsi.ua.es)\n",
        "\n",
        "## Resumen\n",
        "En esta sesión:\n",
        "  * Introducimos el modelo secuencia a secuencia (sequence-to-sequence, o *seq2seq*)    \n",
        "  * Comentaremos problemas complejos que utilizan estas estructuras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsbXE-xaz6yZ"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Dependiendo de la tipología de las entradas y las salidas, podemos formular diferentes escenarios de aprendizaje automático:\n",
        "\n",
        ".\n",
        "\n",
        "![Esquemas con RNN](http://www.dlsi.ua.es/~jgallego/deepraltamira/rnn_diagrams.png)\n",
        "\n",
        ".\n",
        "\n",
        "* Algunos ejemplos de estos tipos de arquitecturas serían:\n",
        "\n",
        " * **Uno a uno:** clasificación o regresión convencional en el cual a una entrada le corresponde una salida.\n",
        " * **Muchos a uno:** clasificación de secuencias, en las cuales queremos asignar una categoria a una secuencia.\n",
        " * **Uno a muchos:** tareas en las cuáles se quiere obtener una secuencia a partir de una única entrada. Ejemplo: descripción automática de imagen.\n",
        " * **Muchos a muchos (desacopladas o no):** tareas en las cuales tanto la entrada como la salida son secuencias. Se puede plantear de diferente forma dependiendo de si las secuencias son desacopladas (traducción automática) o si hay una fuerte relación entre cada elemento de la entrada y la salida (etiquetado gramatical).\n",
        "\n",
        "La flexibilidad para construir las distintas topologías mostradas en la figura anterior se proporcionan a través de siguientes parámetros:\n",
        "\n",
        "* **return_sequences**: cuando está desactivado, sólo devuelve la salida del último instante de tiempo (many to one); cuando está activado, devuelve la salida de todos los instantes (many to many).\n",
        "* **return_state**: indica si se devuelven los estados recurrentes de las neuronas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMmBOfOIWvTI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0298cdd6-5270-430a-b72c-6533ae752a1d"
      },
      "source": [
        "%%capture --no-stdout\n",
        "import tensorflow as tf\n",
        "\n",
        "# La entrada a una red recurrente tiene dos dimensiones:\n",
        "# - El número de instantes de tiempo (None para indicar que variable)\n",
        "# - Número de características de entrada\n",
        "T = 100\n",
        "caracteristicas = 2\n",
        "capa_entrada = tf.keras.layers.Input(shape=(T, caracteristicas))\n",
        "\n",
        "# LSTM\n",
        "x0 = tf.keras.layers.LSTM(64)(capa_entrada)\n",
        "x1 = tf.keras.layers.LSTM(64,return_state=True)(capa_entrada)\n",
        "print('LSTM: ' + str(x0))\n",
        "print('LSTM + return_state: ' + str(x1))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM: Tensor(\"lstm_12/Identity:0\", shape=(None, 64), dtype=float32)\n",
            "LSTM + return_state: [<tf.Tensor 'lstm_13/Identity:0' shape=(None, 64) dtype=float32>, <tf.Tensor 'lstm_13/Identity_1:0' shape=(None, 64) dtype=float32>, <tf.Tensor 'lstm_13/Identity_2:0' shape=(None, 64) dtype=float32>]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j03Y0Nnd1Am-"
      },
      "source": [
        "## Arquitectura secuencia a secuencia\n",
        "\n",
        "De entre todos los enfoques mostrados al principio de la sesión, el más flexible es el de secuencia a secuencia desacoplado (*sequence to sequence*, o *seq2seq*). Típicamente, se implementa con una arquitectura encoder-decoder:\n",
        "\n",
        "![texto alternativo](https://docs.google.com/uc?id=1poBXbaLFiEN0IPZtR0IIbySEnSLGRvih)\n",
        "\n",
        "Esta arquitectura cuenta con dos redes neuronales independientes:\n",
        "* El **encoder** es una red recurrente que se encarga de procesar la entrada elemento a elemento, almacenando en su estado interno una codificación compacta y representativa de la información procesada hasta el momento. Al estado interno de las neuronas del encoder se le llama *context vector* o *thought vector*.\n",
        "* El **decoder** es otra red recurrente que recibe el *context vector* de la última etapa del encoder. En cada paso, predice un elemento del dominio de salida, utilizando el estado interno de la red recurrente y el último elemento predicho. El proceso acaba cuando se emite el elemento especial *EOS* (end of sentence).\n",
        "\n",
        "![texto alternativo](https://devblogs.nvidia.com/wp-content/uploads/2015/06/Figure6_summary_vector_space.png)\n",
        "\n",
        "Esta arquitectura se entrena de manera conjunta, para lo cual tan sólo hacen falta pares de secuencias de entrada y salida, sin necesidad de proporcionar ningun tipo de información acerca de la relación entre los elementos de las mismas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKJXIONTNW1s"
      },
      "source": [
        "## Ejemplo simple de encoder-decoder en Keras (con Embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbE3hEH5Lu8g"
      },
      "source": [
        "En este ejemplo vamos a realizar un ejercicio sin propósito: dada una secuencia de entrada, queremos replicarla en la salida de forma desacoplada. Es decir, primero utilizaremos un encoder para procesar toda la entrada, seguido de un decoder que debe replicar la secuencia a partir del *context vector*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-jomkbQMdPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdaab18d-72d5-4fd5-f55e-1f3dd42ad2d1"
      },
      "source": [
        "\"\"\"\n",
        "Vamos a crear el alfabeto del problema\n",
        "\"\"\"\n",
        "\n",
        "# De palabra a numero\n",
        "word_to_int = {}\n",
        "word_to_int[\"<PAD>\"] = 0\n",
        "word_to_int[\"<SOS>\"] = 1\n",
        "word_to_int[\"<EOS>\"] = 2\n",
        "word_to_int[\"0\"] = 3\n",
        "word_to_int[\"1\"] = 4\n",
        "\n",
        "# De numero a palabra\n",
        "int_to_word = dict([(value, key) for (key, value) in word_to_int.items()])\n",
        "\n",
        "tam_alfabeto = len(word_to_int)\n",
        "\n",
        "print('Alfabeto de tamaño {}'.format( tam_alfabeto ))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alfabeto de tamaño 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcEPg2j0M4Fv"
      },
      "source": [
        "Vamos a crear ahora el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foQHjYuaNbTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc80e63e-1583-4c5c-f5bd-b3007199a362"
      },
      "source": [
        "%%capture --no-stdout\n",
        "import tensorflow as tf\n",
        "\n",
        "# Para este enfoque, merece la pena definir los hiper-parámetros en variables\n",
        "# Especialmente cuando hay que tener el mismo valor en sitios distintos\n",
        "neuronas_rnn = 8\n",
        "dim_embedding = 2\n",
        "\n",
        "# Encoder\n",
        "# - Consta de un Embedding y un LSTM\n",
        "# - Crea un context vector a partir de los últimos estados internos del LSTM\n",
        "\n",
        "entrada_encoder = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "embedding_encoder = tf.keras.layers.Embedding(input_dim=tam_alfabeto,\n",
        "                              output_dim=dim_embedding)(entrada_encoder) # Capa embedding\n",
        "\n",
        "salida_encoder, encoder_h, encoder_c = tf.keras.layers.LSTM(neuronas_rnn,\n",
        "                                             return_state=True)(embedding_encoder) # Devolvemos los estados\n",
        "\n",
        "context_vector = [encoder_h, encoder_c] # Creamos el `context vector`\n",
        "\n",
        "# OJO: El `encoder_outputs` no se usa\n",
        "\n",
        "# Decoder\n",
        "# - Consta de una entrada con Embedding\n",
        "# - El LSTM procesa la entrada del decoder pero su estado inicial es el del context vector\n",
        "# - La salida es una predicción de cada posible categoria\n",
        "\n",
        "entrada_decoder = tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "embedding_decoder = tf.keras.layers.Embedding(input_dim=tam_alfabeto,\n",
        "                              output_dim=dim_embedding)(entrada_decoder)\n",
        "\n",
        "salida_decoder, _, _ = tf.keras.layers.LSTM(neuronas_rnn,\n",
        "                             return_sequences=True,\n",
        "                             return_state=True)(embedding_decoder,\n",
        "                                                initial_state=context_vector) # El `context vector` es el estado inicial del encoder\n",
        "\n",
        "# OJO: El último estado del decoder no se usa\n",
        "\n",
        "# Clasificación\n",
        "# - El decoder es finalmente conectado a una capa para clasificar\n",
        "# - Tantas neuronas como palabras y activación softmax\n",
        "\n",
        "clasificacion = tf.keras.layers.Dense(tam_alfabeto, activation='softmax')(salida_decoder)\n",
        "\n",
        "# Creamos el modelo:\n",
        "# - Dos entradas (entrada del encoder y entrada del decoder)\n",
        "\n",
        "model_embed = tf.keras.models.Model([entrada_encoder, entrada_decoder], clasificacion)\n",
        "\n",
        "model_embed.compile(optimizer='rmsprop',\n",
        "                    loss='categorical_crossentropy')\n",
        "\n",
        "model_embed.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 2)      10          input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 2)      10          input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, 8), (None, 8 352         embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, None, 8), (N 352         embedding_1[0][0]                \n",
            "                                                                 lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 5)      45          lstm_5[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 769\n",
            "Trainable params: 769\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvjdcmkNhSTC"
      },
      "source": [
        "Vamos a definir ahora la función generadora teniendo en cuenta que los lotes estarán compuestos por tres componentes:\n",
        "* **Entrada del encoder (X)**: palabras proporcionadas como entrada del problema\n",
        "* **Entrada del decoder (T)**: comenzando por *SOS* y después la predicción en el instante anterior\n",
        "* **Salida del decoder (Y)**: secuencia de salida esperada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUKzV7ANOats"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Cada lote estará compuesto por tres vectores de tamaño `tam_lote`\n",
        "def generador_embed(tam_lote):\n",
        "  while True:\n",
        "    X = [] # Encoder input\n",
        "    T = [] # Decoder input\n",
        "    Y = [] # Decoder output\n",
        "\n",
        "    # Generamos secuencia de 0's y 1's aleatorias\n",
        "    for _ in range(tam_lote):\n",
        "      x = [random.choice(['0','1']) for _ in range(random.randint(1,5)) ]\n",
        "      t = ['<SOS>'] + x   # La entrada del decoder empieza por SOS\n",
        "      y = x + ['<EOS>']   # La última predicción del decoder es EOS\n",
        "\n",
        "      X.append(x)\n",
        "      T.append(t)\n",
        "      Y.append(y)\n",
        "\n",
        "    # Prepare data\n",
        "    longitud_maxima = max([len(n) for n in X])\n",
        "\n",
        "    X_lote = np.zeros((tam_lote, longitud_maxima),\n",
        "                         dtype=np.int32)\n",
        "    T_lote = np.zeros((tam_lote, longitud_maxima+1), # ¿ +1 ?\n",
        "                         dtype=np.int32)\n",
        "    Y_lote = np.zeros((tam_lote, longitud_maxima+1, tam_alfabeto), # ¿por qué la última dimension?\n",
        "                          dtype=np.int32)\n",
        "\n",
        "    for idx_x, x in enumerate(X):\n",
        "      for idx_p, palabra in enumerate(x):\n",
        "          X_lote[idx_x][idx_p] = word_to_int[palabra]\n",
        "\n",
        "    for idx_t, t in enumerate(T):\n",
        "      for idx_p, palabra in enumerate(t):\n",
        "          T_lote[idx_t][idx_p] = word_to_int[palabra]\n",
        "\n",
        "    for idx_y, y in enumerate(Y):\n",
        "      for idx_p, palabra in enumerate(y):\n",
        "          Y_lote[idx_y][idx_p][word_to_int[palabra]] = 1.\n",
        "\n",
        "\n",
        "    yield [X_lote, T_lote], Y_lote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jzSkqvBiSy-"
      },
      "source": [
        "Podemos inspeccionar una salida típica de la función generadora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcGzaTDfiVP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb67d54-5bc3-4622-9fbe-b38c11215bfb"
      },
      "source": [
        "g = generador_embed(16)\n",
        "\n",
        "[x_i, t_i], y_i = next(g)\n",
        "\n",
        "print('X:', x_i.shape)\n",
        "print('T:', t_i.shape)\n",
        "print('Y:', y_i.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: (16, 5)\n",
            "T: (16, 6)\n",
            "Y: (16, 6, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHvwpbrBiuOA"
      },
      "source": [
        "Con el modelo de red definido y con una función generadora podemos entrenar la red:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNDep81BOwHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0eed89-488c-4a38-9566-46a330643710"
      },
      "source": [
        "%%capture --no-stdout\n",
        "\n",
        "TAM_LOTE = 16\n",
        "generadora_training = generador_embed(TAM_LOTE)\n",
        "\n",
        "# Entrenamiento\n",
        "print('Entrenamos la red...')\n",
        "history = model_embed.fit_generator(generadora_training,\n",
        "                                    steps_per_epoch=200,\n",
        "                                    epochs=30,\n",
        "                                    verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entrenamos la red...\n",
            "WARNING:tensorflow:From <ipython-input-6-349e0e01ecb5>:10: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/30\n",
            "200/200 - 1s - loss: 0.8470\n",
            "Epoch 2/30\n",
            "200/200 - 1s - loss: 0.6709\n",
            "Epoch 3/30\n",
            "200/200 - 1s - loss: 0.5896\n",
            "Epoch 4/30\n",
            "200/200 - 1s - loss: 0.4956\n",
            "Epoch 5/30\n",
            "200/200 - 1s - loss: 0.4534\n",
            "Epoch 6/30\n",
            "200/200 - 1s - loss: 0.4233\n",
            "Epoch 7/30\n",
            "200/200 - 1s - loss: 0.3994\n",
            "Epoch 8/30\n",
            "200/200 - 1s - loss: 0.3886\n",
            "Epoch 9/30\n",
            "200/200 - 1s - loss: 0.3728\n",
            "Epoch 10/30\n",
            "200/200 - 1s - loss: 0.3633\n",
            "Epoch 11/30\n",
            "200/200 - 1s - loss: 0.3194\n",
            "Epoch 12/30\n",
            "200/200 - 1s - loss: 0.2803\n",
            "Epoch 13/30\n",
            "200/200 - 1s - loss: 0.2377\n",
            "Epoch 14/30\n",
            "200/200 - 1s - loss: 0.1844\n",
            "Epoch 15/30\n",
            "200/200 - 1s - loss: 0.1504\n",
            "Epoch 16/30\n",
            "200/200 - 1s - loss: 0.1234\n",
            "Epoch 17/30\n",
            "200/200 - 1s - loss: 0.1036\n",
            "Epoch 18/30\n",
            "200/200 - 1s - loss: 0.0889\n",
            "Epoch 19/30\n",
            "200/200 - 1s - loss: 0.0758\n",
            "Epoch 20/30\n",
            "200/200 - 1s - loss: 0.0662\n",
            "Epoch 21/30\n",
            "200/200 - 1s - loss: 0.0593\n",
            "Epoch 22/30\n",
            "200/200 - 1s - loss: 0.0552\n",
            "Epoch 23/30\n",
            "200/200 - 1s - loss: 0.0468\n",
            "Epoch 24/30\n",
            "200/200 - 1s - loss: 0.0437\n",
            "Epoch 25/30\n",
            "200/200 - 1s - loss: 0.0382\n",
            "Epoch 26/30\n",
            "200/200 - 1s - loss: 0.0344\n",
            "Epoch 27/30\n",
            "200/200 - 1s - loss: 0.0340\n",
            "Epoch 28/30\n",
            "200/200 - 1s - loss: 0.0318\n",
            "Epoch 29/30\n",
            "200/200 - 1s - loss: 0.0286\n",
            "Epoch 30/30\n",
            "200/200 - 1s - loss: 0.0262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9QyvQ33i-IP"
      },
      "source": [
        "Vamos a realizar también el código necesario para realizar una predicción sobre una secuencia de entrada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6gelKK3O2UO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d28c8e-fc5c-46d9-d908-524055df3bb4"
      },
      "source": [
        "# -----------------------------------------------------------------\n",
        "# La función `prediccion` recibe una secuencia y construye\n",
        "# tanto la entrada del encoder como la entrada del decoder\n",
        "\n",
        "def prediccion(secuencia):\n",
        "  x = np.zeros((1, len(secuencia)), dtype=np.int32)\n",
        "  t = np.zeros((1, len(secuencia)+1), dtype=np.int32)\n",
        "\n",
        "  t[0][0] = word_to_int['<SOS>']\n",
        "  for idx_p, palabra in enumerate(secuencia):\n",
        "    x[0][idx_p]   = word_to_int[palabra]\n",
        "    t[0][idx_p+1] = word_to_int[palabra] # ¿ +1 ?\n",
        "\n",
        "  prediccion = model_embed.predict([x,t], verbose=0)[0]\n",
        "  secuencia_predicha = [int_to_word[word] for word in np.argmax(prediccion,axis=1)]\n",
        "\n",
        "  print('Secuencia de entrada:',secuencia,'\\t','Secuencia de salida:', secuencia_predicha)\n",
        "\n",
        "\n",
        "\n",
        "# Vamos a evaluar la siguiente secuencias\n",
        "prediccion(['0'])\n",
        "prediccion(['1'])\n",
        "prediccion(['0','0'])\n",
        "prediccion(['0','1'])\n",
        "prediccion(['1','0'])\n",
        "prediccion(['1','1'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Secuencia de entrada: ['0'] \t Secuencia de salida: ['1', '1']\n",
            "Secuencia de entrada: ['1'] \t Secuencia de salida: ['1', '0']\n",
            "Secuencia de entrada: ['0', '0'] \t Secuencia de salida: ['0', '1', '<EOS>']\n",
            "Secuencia de entrada: ['0', '1'] \t Secuencia de salida: ['0', '1', '<EOS>']\n",
            "Secuencia de entrada: ['1', '0'] \t Secuencia de salida: ['1', '0', '<EOS>']\n",
            "Secuencia de entrada: ['1', '1'] \t Secuencia de salida: ['1', '0', '0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbF_oTljfwg"
      },
      "source": [
        "Como hemos utilizado una capa de *Embedding* podemos inspeccionar dónde se han situado los elementos del vocabulario en el espacio latente de dos dimensiones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8FqedLbPWS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415281a8-f975-4e09-8bd1-e1dbb864b46b"
      },
      "source": [
        "\n",
        "# Creamos el modelo de embedding\n",
        "model_matriz_embedding = tf.keras.models.Model(entrada_encoder,embedding_encoder)\n",
        "\n",
        "# Comprobamos el embedding sobre este modelo\n",
        "codificacion_numerica = [0,1,2,3,4]\n",
        "vectores_embedding = model_matriz_embedding.predict(np.asarray([codificacion_numerica]))\n",
        "\n",
        "print('Representación de: ' + str([int_to_word[idx] for idx in codificacion_numerica]))\n",
        "print(vectores_embedding)\n",
        "\n",
        "# Pintamos la representación vectorial con matplotlib\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Obtenemos las dos dimensiones por separado\n",
        "x,y = vectores_embedding[0].T\n",
        "plt.scatter(x,y)\n",
        "\n",
        "# Las pintamos junto a su etiqueta\n",
        "for idx, _ in enumerate(x):\n",
        "  plt.annotate(int_to_word[idx],\n",
        "               (x[idx], y[idx]))\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Representación de: ['<PAD>', '<SOS>', '<EOS>', '0', '1']\n",
            "[[[ 0.01709491 -0.0917111 ]\n",
            "  [ 0.02717096 -0.02154033]\n",
            "  [ 0.02824816  0.01003578]\n",
            "  [ 0.3412773   0.9308337 ]\n",
            "  [ 0.94898736  0.07174742]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amFgGDeWQbdC"
      },
      "source": [
        "\n",
        "## Ejemplos de tareas (I)\n",
        "\n",
        "Una ventaja de esta arquitectura es que sirve para multitud de tareas que en el pasado se desarrollaban de forma independiente. Aunque cada tarea tiene sus particularidades, esta formulación similar permite que avances significativos en un campo sean generalizables a otro (como veremos más adelante con los modelos de atención).\n",
        "\n",
        "La única restricción para llevar a cabo la mayoría de estas tareas son de índole logístico: ¿tenemos suficientes datos para aprender dicha tarea? ¿tenemos suficiente capacidad computacional para llevar a cabo el entrenamiento?\n",
        "\n",
        "\n",
        "### Modelos de conversación\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://cdn-images-1.medium.com/max/1585/1*sO-SP58T4brE9EHazHSeGA.png)\n",
        "\n",
        ".\n",
        "\n",
        "### Traducción automática\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://www.wncc-iitb.org/images/semantics.png)\n",
        "\n",
        ".\n",
        "\n",
        "### Resumen de texto\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://cdn-images-1.medium.com/max/1600/1*Cu49wPEpWJPoI0a5AV9Q1Q.png)\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CuU9UEMNBH6"
      },
      "source": [
        "### Ejemplos de tareas (II)\n",
        "\n",
        "Se puede mantener este enfoque añadiendo una red convolucional que se encargue de aprender las características adecuadas para la tarea en cuestión. La fase de encoding puede ser o bien una simple red convolucional o una combinación de convolucional y recurrente. El segundo caso es especialmente util cuando la entrada, aun proveniendo de un domino gráfico, tiene también una componente secuencial.\n",
        "\n",
        "Básicamente, estas arquitecturas se emplean en tareas cuya formulación es similar a la que se podría resolver mediante un enfoque encoder-decoder convencional pero en las cuales algún tipo de elemento de entrada es una imagen.\n",
        "\n",
        "#### Descripción visual automática\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://docs.google.com/uc?id=1Q1KbFzrfS7qXIdR3zN7SkcabUwtVm2Rj)\n",
        "\n",
        ".\n",
        "\n",
        "#### *Visual Question Answering*  categórica\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://drive.google.com/uc?id=1hzWsAFmdKSku105HqYuExSsvqfMGj5ie)\n",
        "\n",
        ".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gFnoUwANpuN"
      },
      "source": [
        "# Contenido adicional de la sesión\n",
        "\n",
        "## Modelos de atención\n",
        "\n",
        "A menudo, la etapa de codificación es muy compleja. Es por ello que el context vector es insuficiente para capturar la información necesaria para toda la etapa de decodificación. Para paliar este fenómeno, se utilizan los **modelos de atención**.\n",
        "\n",
        "Un modelo de atención es una estructura neuronal que complementa el enfoque secuencia a secuencia. En cada paso de decodificación, el modelo de atención asigna un peso a cada uno de los elementos de la etapa de codificación; es decir, ayuda a saber en qué parte de la entrada debe el decodificador *poner su atención* en cada paso:\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_mechanism.jpg)\n",
        "\n",
        ".\n",
        "\n",
        "Como subproducto de este proceso, el modelo aprende de manera suavizada tanto la tarea en sí como el alineamiento subyacente:\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://github.com/tensorflow/nmt/raw/master/nmt/g3doc/img/attention_vis.jpg)\n",
        "\n",
        ".\n",
        "\n",
        "También se pueden usar cuando la entrada es una imagen. La diferencia es que, en este caso, la atención se centra en zonas de la imagen en la que prestar atención a la hora de predecir el siguiente elemento.\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://drive.google.com/uc?id=1hi2OaCIxhfLJZaKdebkNVj-vqouXNZ3n)\n",
        "\n",
        ".\n",
        "\n",
        "![texto alternativo](https://drive.google.com/uc?id=1osFmAYRxf-mx3oj6PoBmWaA0i7el1cam)\n",
        "\n",
        ".\n",
        "\n",
        "Los modelos de atención representan el estado del arte en las tareas de secuencia a secuencia implementadas con un enfoque encoder-decoder, y son imprescindibles para construir exitosamente modelos neuronales para problemas complejos.\n",
        "\n",
        "### Modelo de atención en Keras\n",
        "\n",
        "Actualmente, Keras no incorpora modelos de atención en sus implementaciones base. Para poder utilizar este tipo de esquemas, hay que hacerlo combinando los elementos que Keras sí incorpora.\n",
        "\n",
        "* [Tutorial para incorporar atención en Keras](https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html).\n",
        "\n",
        "**UPDATE JULIO 2021**: Actualmente, tf.keras ya incluye modelos de atención en el framework: [capas de Atencion en Keras](https://keras.io/api/layers/attention_layers/attention/).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTeHTjZDnlPQ"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "En la actualidad las RNN están perdiendo relevancia en favor de la arquitectura **Transformer**, en la cual la recurrencia se sustituye por modelos de atención específicos y representaciones de entrada que codifican el orden de los datos de forma implícita.\n",
        "\n",
        "  * [Explicación ilustrada de la arquitectura Transformer](http://jalammar.github.io/illustrated-transformer/)"
      ]
    }
  ]
}