{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCT9SdRt1sv-"
      },
      "source": [
        "<center><img src=\"https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/keras-tf-logo.png\" height=\"200\"></center>\n",
        "\n",
        "# 1.6 Deep Learning con tf.Keras (Keras + TensorFlow)\n",
        "\n",
        "Profesor: Juan Ramón Rico (<juanramonrico@ua.es>)\n",
        "\n",
        "Los ejemplos que se presentarán a continuación están basados en la documentación introductoria de [Keras](https://keras.io/).\n",
        "\n",
        "## Resumen\n",
        "---\n",
        "**tf.keras** es un paquete que permite la creación y prueba de redes neuronales avanzadas (Deep Learning). Tiene una sintaxis sencilla que permite modelar rápido. Desde 2019 fue integrado complementamente en el desarrollo de [Tensorflow v2.x](https://www.tensorflow.org/) de Google y forma parte de él.\n",
        "- Documentación <https://keras.io/> y <https://www.tensorflow.org/tutorials>\n",
        "- Tutorial de inicio rápido <https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/>\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5KtUOqHnl6o"
      },
      "source": [
        "# Redes neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaM9lJHXoAMe"
      },
      "source": [
        "## Neurona de un ser vivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC8JUESpoC8o"
      },
      "source": [
        "<center><img src=\"https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/artificial_vs_biological_neural_cell.png\" height=\"600\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcivneyopT4D"
      },
      "source": [
        "## Perceptrón multicapa\n",
        "\n",
        "Conocido normalmente por sus siglas en inglés MLP (Multilayer Perceptron)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_AFR1sbpW-3"
      },
      "source": [
        "\n",
        "<center><img src=\"https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/mlp.png\" height=\"300\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHQv3hXenseP"
      },
      "source": [
        "## Pricipales topologías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JP2DTlAnseS"
      },
      "source": [
        "<center><img src=\"https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/ann_topologies_basic.png\" ></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG9UX8e7mHrS"
      },
      "source": [
        "## Cronología de aportaciones destacables en las redes neuronales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8XhAZ2CmHrZ"
      },
      "source": [
        "<center><img src=\"https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/dl_timeline.png\" ></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnf-_kCg02_P"
      },
      "source": [
        "> [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) (británico) psicólogo cognitivo e informático. Destaca por sus aportaciones en las redes neuronales artificiales desde los años 80 hasta la actualidad (MLP, back-propagation, Bolzmann Machine Network, Deep Belief Network, Dropout, Capsule Network). Desde 2013 divide su tiempo trabajando para Google ([Google Brain](https://en.wikipedia.org/wiki/Google_Brain)) y la Universidad de Toronto.\n",
        "\n",
        " <center><img src=\"https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/hinton.png\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GfqTzSbdYh-"
      },
      "source": [
        "# tf.keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfn9O9TdqL60"
      },
      "source": [
        "[Keras](https://keras.io/) era una API de redes neuronales de alto nivel, escrita en Python y capaz de ejecutarse sobre otros paquetes de bajo nivel como [TensorFlow](https://www.tensorflow.org/) (Google), [CNTK](https://www.microsoft.com/en-us/cognitive-toolkit/) (Microsoft) o [Theano](http://deeplearning.net/software/theano/) (Université de Montréal). Fue desarrollado con el objetivo de permitir una rápida experimentación. Poder pasar de la idea al resultado en el menor tiempo posible es clave para realizar una buena investigación. Fue uno de los paquetas más utilizados, y puede que el que más, para diseñar redes neuronales. Desde 2015 al 2019 `Tensorflow` fue integrando parte de su API en sus diferentes versiones hasta que en 2019 con la versión 2 de `TensorFlow` integró completamente a `Keras` y a partir de entonces es un módulo de TF 2.x. Por ello se le conoce como `tf.keras`.\n",
        "\n",
        "Utiliza **`tf.keras`** si necesitas una biblioteca de Deep Learning que:\n",
        "\n",
        "- Permite un prototipado fácil y rápido (a través de la facilidad de uso, modularidad y extensibilidad).\n",
        "- Soporte tanto redes convolucionales como redes recurrentes, así como combinaciones de ambas.\n",
        "- Se ejecute en la CPU y la GPU.\n",
        "\n",
        "Basado en los principios de:\n",
        "\n",
        "- **Facilidad de uso**  Keras es una API diseñada para seres humanos, no para máquinas. Pone la experiencia del usuario en primer plano. Keras sigue las mejores prácticas para reducir la carga cognitiva: ofrece APIs consistentes y simples, minimiza el número de acciones de usuario requeridas para casos de uso común, y proporciona una retroalimentación clara y procesable sobre el error del usuario.\n",
        "\n",
        "- **Modularidad**: Un modelo se entiende como una secuencia o un gráfico de módulos independientes, totalmente configurables, que pueden conectarse entre sí con las mínimas restricciones posibles. En particular, las capas neuronales, las funciones de coste, los optimizadores, los esquemas de inicialización, las funciones de activación y los esquemas de regularización son módulos independientes que puede combinar para crear nuevos modelos.\n",
        "\n",
        "- **Permite una extensión fácil**:  Los nuevos módulos son fáciles de añadir (como nuevas clases y funciones), y los existentes proporcionan amplios ejemplos que se intregran con todo el ecosistema de productos de `TensorFlow`. El poder crear fácilmente nuevos módulos permite una expresividad total, haciendo que sea adecuado para la investigación avanzada. Permite tres estilos de programación: secuencial (fácil), funcional (medio) y subclases (avanzado).\n",
        "\n",
        "- **Trabaja con Python**: No hay archivos de configuración de modelos separados en formato declarativo. Los modelos se describen en código `Python`, que es compacto, más fácil de depurar y permite su extensión fácilmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2NU5Fz85MBn"
      },
      "source": [
        "# Ejemplos básicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvtHCkKxcKBb"
      },
      "source": [
        "## Trabajar con tf.keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKAgEX2BJwx7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(f'TensorFlow versión : {tf.__version__}')\n",
        "print(f'tf.keras versión: {tf.keras.__version__}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLcTc2twg3PX"
      },
      "source": [
        "## Copiar los datos a Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XxIpWmleHQG"
      },
      "source": [
        "# Hay que copiar los archivos de ejemplo\n",
        "!wget https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/data/basic_data.zip\n",
        "!unzip basic_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESZsaHFj2GgU"
      },
      "source": [
        "## Crear y entrenar un modelo\n",
        "\n",
        "Los modelos de Keras se entrenan con matrices de datos de entrada y etiquetas de `Numpy` o tablas de datos de `Pandas`. Para entrenar un modelo se utiliza la función `fit` ([más información](https://keras.io/models/sequential/))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPFsPJLYa-D4"
      },
      "source": [
        "En este primer ejemplo cargaremos un fichero de datos CSV con dos clases objetivo (si está enfermo o no) y entrenaremos un MLP con todos los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eaLhinE8r0w"
      },
      "source": [
        "## MLP para clasificación binaria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW-4OLZE6gRh"
      },
      "source": [
        "Este primer planteamiento simplemente es para comprobar si la implementación de nuestra red es correcta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diuVJi5oI-Pr"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras import Sequential, layers\n",
        "\n",
        "# Clasificación binaria con el fichero 'diabetes_01.csv'\n",
        "data = pd.read_csv('./basic_data/diabetes_01.csv')\n",
        "pairs = {'int64':'int32', 'float64':'float32'}\n",
        "for i in data.columns:\n",
        "  data[i]= data[i].astype(pairs.get(str(data[i].dtype), data[i].dtype))\n",
        "\n",
        "# Selección de atributos y variable objetivo\n",
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:,-1]\n",
        "\n",
        "# Para un modelo con una entrada de datos numéricos y para clasificar 2 clases (binary classification):\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(32, activation='relu', input_dim=X.shape[1]))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo, iterando con los datos en grupos de 32 muestras\n",
        "model.fit(X, y, epochs=50, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VisNBvjIbXHW"
      },
      "source": [
        "En el ejemplo anterior se entrena sobre todos los datos disponibles con lo que la tasa de aciertos finales no sabemos hasta que punto es fiable.\n",
        "\n",
        "Para ello, vamos a dividir los datos en un par de conjuntos para poder entrenar con uno y testear con el otro.\n",
        "\n",
        "Vamos a evitar el sobre-entrenamiento (overfitting) con una función llamada `Dropout` que se explicará con detenimiento en la siguiente sesión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiPBloWl8ukJ"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import Sequential, layers\n",
        "\n",
        "# Clasificación binaria con el fichero 'diabete_01.csv'\n",
        "data = pd.read_csv('./basic_data/diabetes_01.csv')\n",
        "\n",
        "pairs = {'int64':'int32', 'float64':'float32'}\n",
        "for i in data.columns:\n",
        "  data[i]= data[i].astype(pairs.get(str(data[i].dtype), data[i].dtype))\n",
        "\n",
        "# Selección de atributos y variable objetivo\n",
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:,-1]\n",
        "\n",
        "# Dividir los datos en entrenamiento y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=123)\n",
        "\n",
        "# Scalar valores de las columnas (atributos)\n",
        "# sc =  StandardScaler()\n",
        "# sc.fit(X_train)\n",
        "# X_train = sc.transform(X_train)\n",
        "# X_test = sc.transform(X_test)\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Dense(32, input_dim=X_train.shape[1], activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=70, batch_size=8)\n",
        "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
        "print(f'Test accuracy {accuracy:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYQ4HMB2s0if"
      },
      "source": [
        "## Integración de `Keras` en `scikit-learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasta hace pocas versiones estaba soportado directamente por tf2 pero ahora lo soporta un proyectos externo `scikeras` (https://github.com/adriangb/scikeras).\n",
        "\n",
        "Así que procederemos a instalarlo.\n"
      ],
      "metadata": {
        "id": "3wrcO-KS45k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras -U"
      ],
      "metadata": {
        "id": "4WPIVNMK4iaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFJAqaPJq4H6"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras import Sequential, layers\n",
        "\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
        "\n",
        "# Clasificación binaria con el fichero 'diabete_01.csv'\n",
        "data = pd.read_csv('./basic_data/diabetes_01.csv')\n",
        "\n",
        "pairs = {'int64':'int32', 'float64':'float32'}\n",
        "for i in data.columns:\n",
        "  data[i]= data[i].astype(pairs.get(str(data[i].dtype), data[i].dtype))\n",
        "\n",
        "# Selección de atributos y variable objetivo\n",
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:,-1]\n",
        "\n",
        "def build_model(input_dim):\n",
        "  model = Sequential([\n",
        "      layers.Dense(32, input_dim=X.shape[1], activation='relu'),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(16, activation='relu'),\n",
        "      layers.Dropout(0.3),\n",
        "      layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = build_model(X.shape[1])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIl33so1g8Y5"
      },
      "source": [
        "%%capture --no-stdout\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "names = [\"MLP Keras\",\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
        "         \"Decision Tree\", \"Random Forest\", \"MLP\", \"AdaBoost\",\n",
        "         \"Naive Bayes\"]\n",
        "\n",
        "models = [\n",
        "    KerasClassifier(model=build_model, input_dim=X.shape[1], batch_size=32, epochs=75, verbose=0),\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel='linear'),\n",
        "    SVC(kernel='rbf'),\n",
        "    DecisionTreeClassifier(random_state=123),\n",
        "    RandomForestClassifier(random_state=123),\n",
        "    MLPClassifier(random_state=123),\n",
        "    AdaBoostClassifier(random_state=123),\n",
        "    GaussianNB()\n",
        "]\n",
        "\n",
        "\n",
        "cv = KFold(n_splits=10, random_state=123, shuffle=True)\n",
        "# iterate over classifiers\n",
        "for name, model in zip(names, models):\n",
        "  pipe = Pipeline([('scale', StandardScaler()), (name, model)])\n",
        "  results = np.round(cross_val_score(pipe, X, y, cv=cv),2)\n",
        "  print(f'{name:20s} media: {results.mean():.2} resultados: {results}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRQV2yqj68uf"
      },
      "source": [
        "## Clasificación con Multilayer Perceptron (MLP) para diversas clases (softmax):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bV_EbUbbrjg"
      },
      "source": [
        "* Ahora vamos a trabajar de nuevo sobre el conjunto de datos llamado [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
        "\n",
        "* Además tenemos tres categorias (iris-setosa, iris-virginica e iris-versicolor) que convertiremos a identificadores (0, 1 y 2) con `LabelEncoder`;\n",
        "\n",
        "* Las categorías en redes necesitan una codificación binaria excluyente llamada **one hot**, es decir, la representación sería 0 (1,0,0), 1 (0,1,0) y 2 (0,0,1).\n",
        "\n",
        "* En esta ocasión también crearemos un par de conjuntos disjuntos; uno para entrenar y el otro para testear los resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fyhj6aa661q"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import Sequential, layers, optimizers, utils\n",
        "\n",
        "# Clasificación múltiple con el fichero 'iris.csv'\n",
        "label_enc = LabelEncoder()\n",
        "data = pd.read_csv('./basic_data/iris.csv')\n",
        "X = data.iloc[:,:-1]\n",
        "y = label_enc.fit_transform(data.iloc[:,-1])\n",
        "n_classes = len(np.unique(y))\n",
        "\n",
        "# Dividir los datos en entrenamiento y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=123)\n",
        "y_train = utils.to_categorical(y_train)\n",
        "y_test = utils.to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "  # Dense(64) es una capa de conexión completa con 64 neuronas ocultas.\n",
        "  # en la primera capa se tiene que especificar la dimensión de la entrada de datos.\n",
        "  layers.Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(64, activation='relu'),\n",
        "  layers.Dropout(0.3),\n",
        "  layers.Dense(n_classes, activation='softmax') # Número de clases finales\n",
        "])\n",
        "\n",
        "sgd = optimizers.SGD( momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
        "print(f'Test accuracy {accuracy:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l3bYsU--eOl"
      },
      "source": [
        "**Ejercicio**: Probar con el valor de `SGD` por defecto.\n",
        "\n",
        "¿Qué ocurre?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEh6b3q524sD"
      },
      "source": [
        "## MLP para regresión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18NGURmn3Gql"
      },
      "source": [
        "Vamos a mostrar un ejemplo para calcular un valor final. Nos basaremos en el conjunto de datos de `Boston House Prices` incluida en los datos descargados previsamente (`basic_data.zip`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boston house prices dataset\n",
        "\n",
        "**Data Set Characteristics:**  \n",
        "\n",
        "    :Number of Instances: 506\n",
        "\n",
        "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
        "\n",
        "    :Attribute Information (in order):\n",
        "        - CRIM     per capita crime rate by town\n",
        "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "        - INDUS    proportion of non-retail business acres per town\n",
        "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "        - NOX      nitric oxides concentration (parts per 10 million)\n",
        "        - RM       average number of rooms per dwelling\n",
        "        - AGE      proportion of owner-occupied units built prior to 1940\n",
        "        - DIS      weighted distances to five Boston employment centres\n",
        "        - RAD      index of accessibility to radial highways\n",
        "        - TAX      full-value property-tax rate per $10,000\n",
        "        - PTRATIO  pupil-teacher ratio by town\n",
        "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
        "        - LSTAT    % lower status of the population\n",
        "        - MEDV     Median value of owner-occupied homes in $1000's\n",
        "\n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "    :Creator: Harrison, D. and Rubinfeld, D.L."
      ],
      "metadata": {
        "id": "re5r8ZoRB_1s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6pIkzFW3mPR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('basic_data/boston.csv').astype('float32')\n",
        "\n",
        "X, y = data.iloc[:,:-1], data.iloc[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9kzXZvPhq5L"
      },
      "source": [
        "### Prueba con entrenamiento y test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuzw4fd7iDiM"
      },
      "source": [
        "Para esta prueba necesitamos escalar los diferentes indicadores, ya que tienen diferentes rangos de valores (`StandardScaler`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8cfrFQz3BXT"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras import Sequential, layers, optimizers, utils\n",
        "\n",
        "# Para un modelo para calcular una regresión\n",
        "def build_mlp_reg(input_dim):\n",
        "  model = Sequential([\n",
        "    layers.Dense(32, activation='relu', kernel_initializer='normal', input_dim=input_dim),\n",
        "    layers.Dense(1, kernel_initializer='normal')\n",
        "  ])\n",
        "  model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "\n",
        "  return model\n",
        "\n",
        "# Necesitamos ajustar los rangos de las entradas (x) y las salidas (y)\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train_scaler = scaler.fit_transform(X_train)\n",
        "X_test_scaler = scaler.transform(X_test)\n",
        "\n",
        "# Construir el modelo\n",
        "model = build_mlp_reg(X_train.shape[1])\n",
        "\n",
        "# Entrenar el modelo, iterando con los datos en grupos de 8 muestras\n",
        "model.fit(X_train_scaler, y_train, epochs=100, batch_size=8, validation_split=0.1, verbose=2)\n",
        "\n",
        "# Probar el conjunto de test\n",
        "y_pred = model.predict(X_test_scaler)\n",
        "score = metrics.mean_absolute_error(y_test, y_pred)\n",
        "print(f'mean absolute error: {score:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol-26rWZ2Xea"
      },
      "source": [
        "# Suponemos que queremos predecir una nueva muestra\n",
        "df = pd.DataFrame({'Nombres':data.columns[:-1], 'Valores':X_test_scaler[-1]}) # Última muestra de test\n",
        "display(df.T)\n",
        "print(f'Predicción: {y_pred[-1][0]:.2f}')\n",
        "print(f'Valor real: {y_test.iloc[-1]:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmhriloIhv1-"
      },
      "source": [
        "### Validación cruzada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bArocuhn4Mdw"
      },
      "source": [
        "Vamos a comparar los resultados de esta regresión MLP usando una validación cruzada de 10 con los resultados del tema previo.\n",
        "\n",
        "Usaremos el método `KerasRegressor` para comunicar los modelos de `Keras` con la utilidades de `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqtaZdjFe4by"
      },
      "source": [
        "%%capture --no-stdout\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Sequential, layers, optimizers, utils\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn import pipeline, model_selection, preprocessing\n",
        "\n",
        "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
        "         \"Decision Tree\", \"Random Forest\", \"AdaBoost\",\n",
        "         \"MLP\", \"MLP-Keras\"]\n",
        "\n",
        "models = [\n",
        "    KNeighborsRegressor(3),\n",
        "    SVR(kernel='linear'),\n",
        "    SVR(kernel='rbf'),\n",
        "    DecisionTreeRegressor(random_state=123),\n",
        "    RandomForestRegressor(random_state=123),\n",
        "    AdaBoostRegressor(random_state=123),\n",
        "    MLPRegressor(random_state=123),\n",
        "    KerasRegressor(model=build_mlp_reg, input_dim=X_train.shape[1], epochs=100, batch_size=8, verbose=0)\n",
        "]\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# iterate over regressors models\n",
        "cv = model_selection.KFold(n_splits=10, random_state=123, shuffle=True)\n",
        "for name, model in zip(names, models):\n",
        "  pipe = pipeline.Pipeline([('scaler', preprocessing.StandardScaler()),\n",
        "                            ('regressor', model)])\n",
        "  results = -np.round(model_selection.cross_val_score(pipe, X, y, scoring='neg_mean_absolute_error', cv=cv),2)\n",
        "  print(f'{name:20s} media: {results.mean():.2} resultados: {results}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFh79r_T5evo"
      },
      "source": [
        "# Ejemplos avanzados\n",
        "\n",
        "En este enlace, <https://keras.io/examples/>, tenemos más ejemplos completos para comenzar en aplicaciones a diferentes ámbitos:\n",
        "\n",
        "- Visión por ordenador\n",
        "- Procesamiento del lenguaje natural\n",
        "- Datos estructurados\n",
        "- Series temporales\n",
        "- Datos de audio\n",
        "- Aprendizaje profundo generativo\n",
        "- Aprendizaje por refuerzo\n",
        "- Datos gráficos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pvd2iHhfmMe"
      },
      "source": [
        "## Ejemplos de gráficos de puntos 2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO-l6A2uR9Yt"
      },
      "source": [
        "# Gráfico usando Pandas (sepal)\n",
        "data = pd.read_csv('./basic_data/iris.csv')\n",
        "\n",
        "colors = {'Iris-setosa':'blue', 'Iris-versicolor':'red', 'Iris-virginica':'green'}\n",
        "data.plot.scatter(x='sepallength', y='sepalwidth', c=[colors[i] for i in data['class']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVQo7CgsXQma"
      },
      "source": [
        "# Gráfico interactivo usando Altair (sepal)\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "alt.Chart(data).mark_point().encode(x='sepallength', y='sepalwidth', fill='class', shape='class').interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuD3zvKhV0nY"
      },
      "source": [
        "# Gráfico usando Pandas (petal)\n",
        "\n",
        "colors = {'Iris-setosa':'blue', 'Iris-versicolor':'red', 'Iris-virginica':'green'}\n",
        "data.plot.scatter(x='petallength', y='petalwidth', c=[colors[i] for i in data['class']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yza5Sg5cYYu8"
      },
      "source": [
        "# Gráfico interactivo usando Altair (petal)\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "alt.Chart(data).mark_point().encode(x='petallength', y='petalwidth', fill='class', shape='class').interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxG9bRlY39YL"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5ESNcU23_e-"
      },
      "source": [
        "* Introducción intuitiva a las redes neuronales atificiales.\n",
        "\n",
        "* **tf.keras** módulo básico de TensorFlow para comenzar a usar **Deep Learning**.\n",
        "\n",
        "* La red básica es el MLP (Multilayer Perceptron) pero existen multitud de topologías. Las más importantes se estudiarán en las próximas sesiones.\n",
        "\n",
        "* Se han presentado algunos ejemplos con **tf.keras** para solucionar problemas similares a los estudiados en el tema anterior con `sklearn`.\n",
        "\n",
        "* En redes neuronales es habitual tener que **reestructurar de los datos** con `reshape` (`Numpy`) y ajustar la escala de valores con `StandardScaler` (`sklearn`) o similares."
      ]
    }
  ]
}