{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJZKeSEgdcve"
      },
      "source": [
        "# 2.2. Redes neuronales convolucionales\n",
        "\n",
        "* Una red neuronal convolucional (***Convolutional Neural Network*** o **CNN**) es un tipo de red neuronal artificial donde las neuronas imitan los campos receptivos de las neuronas de la corteza visual primaria de un cerebro biológico.\n",
        "\n",
        "* Son una variación de los MLP, sin embargo, debido a que su aplicación se realiza en matrices bidimensionales, funcionan muy bien en tareas de visión artificial, como la clasificación o segmentación de imágenes, entre otras.\n",
        "\n",
        "* [En 1989 Yann LeCun](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) propuso el uso de las CNN para aplicaciones en visión por computador, y un año después publicó la red conocida como [LeNet](https://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network.pdf) para el reconocimiento de dígitos manuscritos.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Deep Learning Timeline](http://www.dlsi.ua.es/~jgallego/deepraltamira/deep_learning_timeline_cnn.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "* Posteriormente se publicaron muchos artículos sobre el uso de CNN pero sin resultados muy destacables, hasta que en [2012, Krizhevsky, Sutskever y Hinton](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), publicaron la red conocida como **AlexNet**, ganando el concurso de clasificación de imágenes ImageNet:\n",
        "\n",
        "<br>\n",
        "\n",
        "![ImagetNet contest](http://www.dlsi.ua.es/~jgallego/deepraltamira/cnn_imagenet.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "* En general, las redes CNN están formadas por una o más capas convolucionales seguidas de una o más capas tipo MLP (aquí llamadas \"capas totalmente conectadas\" o *Fully Connected*).\n",
        "\n",
        "<br>\n",
        "\n",
        "![Red Neuronal Convolucional](http://www.dlsi.ua.es/~jgallego/deepraltamira/cnn_typical.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "* En las capas convolucionales se realiza la extracción de características de la imagen (colores, gradientes, bordes, esquinas, formas, etc.).\n",
        "\n",
        "* Después de cada capa convolucional se suele disminuir su dimensionalidad (*subsampling*) para que las capas más profundas puedan aprender características cada vez más complejas (combinando las características extraídas previamente).\n",
        "\n",
        "<br>\n",
        "\n",
        "![Red Neuronal Convolucional](http://www.dlsi.ua.es/~jgallego/deepraltamira/cnn_feature_hierarchy.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "* En las capas *fully connected* finales se realiza un mapeo no-lineal de las características extraídas de la imagen a las categorías a clasificar.\n",
        "\n",
        "* Este tipo de redes también pueden ser aplicadas para la clasificación de series temporales o señales de audio utilizando **convoluciones 1D**, así como para la clasificación de datos volumétricos usando **convoluciones 3D**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCHTu5IrdiLA"
      },
      "source": [
        "## Operación de convolución\n",
        "\n",
        "* La operación de convolución (convolución discreta) recibe como entrada un array 2D (una imagen) y aplica sobre ella un filtro o ***kernel*** que nos devuelve un mapa con las características extraídas de la imagen original.\n",
        "\n",
        "* La salida de cada neurona convolucional se calcula como:\n",
        "\n",
        "<br>\n",
        "\n",
        "\\begin{equation}\n",
        "    y = f \\Big(b + \\sum K \\otimes x_n \\Big)\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "  * Donde: La salida $y$ es una matriz que se calcula por medio de la combinación lineal de las entradas $x_n$ recibidas de las neuronas en la capa anterior,  operadas con el núcleo o ***kernel*** de convolución $K$ correspondiente, se le añade el bias $b$ y por último se pasa por la función de activación $f$.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![Convolucion](http://www.dlsi.ua.es/~jgallego/deepraltamira/convolution.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "* El resultado obtenido se ha calculado de la forma:\n",
        "\n",
        "\n",
        "        Píxel = 105 * 0  + 102 * -1 + 100 * 0\n",
        "              + 103 * -1 +  99 * 5  + 103 * -1\n",
        "              + 101 * 0  +  98 * -1 + 104 * 0\n",
        "              = 89\n",
        "\n",
        "\n",
        "\n",
        "* El resultado obtenido se guarda en el píxel central de la posición sobre la que se ha aplicado el *kernel*, por este motivo las dimensiones de los kernels suelen ser impares.\n",
        "\n",
        "\n",
        "* La operación de convolución transforma los datos de tal manera que ciertas características (determinadas por el *kernel* utilizado) se resaltan en la imagen de salida.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![Ejemplos de resultados de convolución con distintos kernels](http://www.dlsi.ua.es/~jgallego/deepraltamira/convolution_results.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYBRjmzHnRtI"
      },
      "source": [
        "### Ejemplo de convolución\n",
        "\n",
        "A continuación se incluye un ejemplo de como aplicar una convolución con distintos kernels sobre una imagen.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5quZhwuhdmQD"
      },
      "source": [
        "\"\"\"\n",
        "En primer lugar descargamos la imagen que vamos a utilizar y mostramos\n",
        "los datos de la misma.\n",
        "\"\"\"\n",
        "\n",
        "# Descargamos una imagen de prueba\n",
        "!wget -q http://www.dlsi.ua.es/~jgallego/deepraltamira/sample_lenna.jpg\n",
        "\n",
        "# Importamos las librerías de Matplotlib y de OpenCV\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Leemos la imagen descargada\n",
        "img = cv2.imread('sample_lenna.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Mostramos los datos de la imagen\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "print('Matriz de píxeles de la imagen:')\n",
        "print(img)\n",
        "\n",
        "print('Dimensiones de la imagen:')\n",
        "print(img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVbMzcVdDD1"
      },
      "source": [
        "\"\"\"\n",
        "Y a continuación aplicamos diferentes Kernels sobre esta imagen\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------------\n",
        "def convolve2d_and_show(image, kernel):\n",
        "  out = cv2.filter2D(src=image, kernel=kernel, ddepth=-1)\n",
        "  print(\"Convolución con Kernel:\")\n",
        "  print(kernel)\n",
        "  plt.imshow(out, cmap='gray')\n",
        "  plt.grid(False)\n",
        "  plt.show()\n",
        "\n",
        "kernel = np.array([[5,0,-5], [0,0,0], [-5,0,5]])\n",
        "convolve2d_and_show(img, kernel)\n",
        "\n",
        "kernel = np.array([[-1,-2,-1], [0,0,0], [1,2,1]])\n",
        "convolve2d_and_show(img, kernel)\n",
        "\n",
        "\n",
        "kernel = np.array([[1,0,-1], [2,0,-2], [1,0,-1]])\n",
        "convolve2d_and_show(img, kernel)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svveVHWsVWkH"
      },
      "source": [
        "Pero...\n",
        "\n",
        "¿tenemos que establecer nosotros manualmente los pesos de los kernels que van a usar las convoluciones?\n",
        "\n",
        "No, **los pesos se aprenden** durante el entrenamiento.\n",
        "\n",
        "De esta forma se aprenderán los *kernels* o filtros más adecuados para clasificar los tipos de imágenes suministrados durante el entrenamiento.\n",
        "\n",
        "Este tipo de aprendizaje se denomina \"*feature learning*\" (aprendizaje de características o de representación)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiVOjSOzd7d3"
      },
      "source": [
        "## Capas de convolución\n",
        "\n",
        "* Cada una de las capas de convolución de la red puede aplicar uno o más filtros.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Conjunto de Kernels de una capa convolucional](http://www.dlsi.ua.es/~jgallego/deepraltamira/array_kernels.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "* Cada uno de los filtros de la capa se especializará en detectar un tipo de característica de la imagen de entrada.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Filtros aprendidos](http://www.dlsi.ua.es/~jgallego/deepraltamira/filters.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "* Los mapas de caracteríscas obtenidos se suman, se les añade el *bias* (también aprendido) y al resultado se le aplica la función de activación.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Suma de convoluciones](http://www.dlsi.ua.es/~jgallego/deepraltamira/convolution_addition1.png)\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85yRVZ6JeFqt"
      },
      "source": [
        "## Subsampling\n",
        "\n",
        "* Después de cada capa de convolución normalmente se aplica una operación de subsampling.\n",
        "\n",
        "* Las técnicas más utilizadas para realizar esta operación son **Max-Pooling** y **Average Pooling**.\n",
        "\n",
        "* En ambos casos se aplica un filtro (de dimensiones $w \\times h$) sobre la imagen de entrada y se guarda el máximo (o la media en el caso del *Average Pooling*) de cada región en la matriz de salida.\n",
        "\n",
        "* A continuación se muestra un ejemplo de cómo se aplicaría un filtro de Max-Pooling de tamaño de 2x2:\n",
        "\n",
        "<br>\n",
        "\n",
        "![Max Pooling](http://www.dlsi.ua.es/~jgallego/deepraltamira/max_pooling.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "* El objetivo es reducir el tamaño de la imagen de entrada quedándonos con las características más relevantes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXG4jqaBeJEz"
      },
      "source": [
        "## Jearquía de características\n",
        "\n",
        "* Al aplicar consecutivamente, capa tras capa, operaciones de convolución seguidas de subsampling obtenemos una arquitectura o topología de red como la siguiente:\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![Jerarquía de capas en CNN](http://www.dlsi.ua.es/~jgallego/deepraltamira/cnn_jerarquia.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "* En las primeras capas los filtros solo se aplican sobre una pequeña parte de la imagen.\n",
        "\n",
        "* Pero después de varias operaciones de subsampling los filtros aplicados pueden ver toda la imagen.\n",
        "\n",
        "* Esto crea una jerarquía de características en la que en las primeras capas se aprenden filtros de más bajo nivel (bordes, colores, gradientes, etc.) y progresivamente se van combinando y aprendiendo características de más alto nivel.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Jerarquía de características](http://www.dlsi.ua.es/~jgallego/deepraltamira/cnn_feature_hierarchy_2.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "* Las características extraídas en las últimas capas de convolución han sido depuradas hasta llegar a una serie de características únicas que permitan discriminar la clase de la que se trata la imagen de entrada.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnEftL_GyCAk"
      },
      "source": [
        "## Capa *Fully connected*\n",
        "\n",
        "* Las últimas capas de la CNN se encargan de clasificar las características extraídas de la imagen en una de las posibles categorías.\n",
        "\n",
        "* Estas últimas capas suelen ser de tipo *Fully Connected* (capas totalmente conectadas), que son equivalentes a las Redes Neuronales o MLP que vimos previamente.\n",
        "\n",
        "* Para transformar los mapas de características (que son matrices 2D) en un vector 1D se realiza una operación llamada ***Flatten*** (aplanar).\n",
        "\n",
        "* Esta operación simplemente consiste en redimensionar los mapas de características de salida en un vector 1D:\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![Operación flatten](http://www.dlsi.ua.es/~jgallego/deepraltamira/flatten1.png)\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FS6xi5ryBws"
      },
      "source": [
        "# 2.2.1. Redes CNN con tf.Keras\n",
        "\n",
        "* En tf.Keras disponemos de las clases `Conv2D`, `MaxPooling2D` y `Flatten` para crear una red neuronal convolucional.\n",
        "\n",
        "\n",
        "* La clase [Conv2D](https://keras.io/layers/convolutional/#conv2d)  permite añadir capas convolucionales a la red.\n",
        "\n",
        " * Como parámetros recibe el número de filtros y el tamaño de los kernels, por ejemplo \"`Conv2D(32, (3, 3))`\" crearía una capa con 32 filtros de tamaño 3x3.\n",
        "\n",
        "* [MaxPooling2D](https://keras.io/layers/pooling/#maxpooling2d) añade una capa para aplicar esta operación con el tamaño indicado como parámetro.\n",
        "\n",
        " * Por ejemplo `MaxPooling2D(pool_size=(2, 2))`.\n",
        "\n",
        "* La clase [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/) realiza esta operación a partir de las entradas recibidas, devolviendo un vector 1D.\n",
        "\n",
        "<br>\n",
        "\n",
        "&#10158; A continuación vamos a ver un ejemplo sencillo de cómo clasificar la base de datos MNIST usando una Red Neuronal Convolucional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDisnOkYIAny"
      },
      "source": [
        "\"\"\"\n",
        "En primer lugar descargamos la base de datos y mostramos algunas imágenes\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(1)  # Fijamos la semilla de TF\n",
        "np.random.seed(1)  # Fijamos la semilla\n",
        "\n",
        "# Descargamos la base de datos\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Mostramos algunas imágenes\n",
        "n = 15\n",
        "index = np.random.randint(len(x_train), size=n)\n",
        "plt.figure(figsize=(n*1.5, 1.5))\n",
        "for i in np.arange(n):\n",
        "    ax = plt.subplot(1,n,i+1)\n",
        "    ax.set_title('{} ({})'.format(y_train[index[i]],index[i]))\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    plt.imshow(x_train[index[i]], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# Mostramos las dimensiones de los datos\n",
        "print('Datos para entrenamiento:')\n",
        "print(' - x_train: {}'.format(str(x_train.shape)))\n",
        "print(' - y_train: {}'.format(str(y_train.shape)))\n",
        "print('Datos para evaluación:')\n",
        "print(' - x_test: {}'.format(str(x_test.shape)))\n",
        "print(' - y_test: {}'.format(str(y_test.shape)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-pgjclvOjK7"
      },
      "source": [
        "\"\"\"\n",
        "Preparamos los datos para la red\n",
        "\"\"\"\n",
        "\n",
        "# Redimensionamos para añadir el canal\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        "\n",
        "# Transformamos a decimal\n",
        "x_train = x_train.astype(np.float32)\n",
        "x_test = x_test.astype(np.float32)\n",
        "\n",
        "# Normalizamos entre 0 y 1\n",
        "x_train /= 255.\n",
        "x_test /= 255.\n",
        "\n",
        "# Transformamos las etiquetas a categórico (one-hot)\n",
        "NUM_LABELS = 10\n",
        "y_train  = tf.keras.utils.to_categorical(y_train, NUM_LABELS)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, NUM_LABELS)\n",
        "\n",
        "\n",
        "# Mostramos (de nuevo) las dimensiones de los datos\n",
        "print('Datos para entrenamiento:')\n",
        "print(' - x_train: {}'.format(str(x_train.shape)))\n",
        "print(' - y_train: {}'.format(str(y_train.shape)))\n",
        "print('Datos para evaluación:')\n",
        "print(' - x_test: {}'.format(str(x_test.shape)))\n",
        "print(' - y_test: {}'.format(str(y_test.shape)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "307btIC8gJQK"
      },
      "source": [
        "\"\"\"\n",
        "Definimos la CNN a utilizar y la entrenamos\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Capa convolucional con 8 filtros de tamaño 3x3 seguida de un MaxPooling de 2x2\n",
        "model.add(Conv2D(8, (3, 3), activation='relu', name='conv1', input_shape=x_train.shape[1:]))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Capa convolucional con 4 filtros de tamaño 3x3 seguida de un MaxPooling de 2x2\n",
        "model.add(Conv2D(4, (3, 3), activation='relu', name='conv2'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Capa Fully Connected de salida con función de activación Softmax\n",
        "model.add(Flatten())\n",
        "model.add(Dense(NUM_LABELS, activation='softmax'))\n",
        "\n",
        "# Mostramos el resumen de la red\n",
        "print(model.summary())\n",
        "\n",
        "# La compilamos usando \"categorical crossentropy\" como función de pérdida,\n",
        "# Adam como optimizador y añadimos la métrica accuracy\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
        "\n",
        "# Iniciamos el entrenamiento durante 5 épocas con un tamaño de batch de 32\n",
        "history = model.fit(x_train, y_train, validation_split=0.33,\n",
        "                    batch_size=32, epochs=5, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OufC3GU6W8Lz"
      },
      "source": [
        "\"\"\"\n",
        "Mostramos las curvas de aprendizaje y evaluamos usando el test set\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------\n",
        "def plot_learning_curves(hist):\n",
        "  plt.plot(hist.history['loss'])\n",
        "  plt.plot(hist.history['val_loss'])\n",
        "  plt.title('Curvas de aprendizaje')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Conjunto de entrenamiento', 'Conjunto de validación'], loc='upper right')\n",
        "  plt.show()\n",
        "\n",
        "print('Mostramos las curvas de aprendizaje')\n",
        "plot_learning_curves(history)\n",
        "\n",
        "\n",
        "# Evaluamos usando el test set\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Resultado en el test set:')\n",
        "print('Test loss: {:0.4f}'.format(score[0]))\n",
        "print('Test accuracy: {:0.2f}%'.format(score[1] * 100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQXwX0RWSgbu"
      },
      "source": [
        "\"\"\"\n",
        "Mostramos los filtros aprendidos por la red\n",
        "\"\"\"\n",
        "\n",
        "# --------------------------------\n",
        "def plot_figures(images):\n",
        "  width = images.shape[0]\n",
        "  n_filters = images.shape[2]\n",
        "  plt.figure(figsize=(1.5 * n_filters, 1.5))\n",
        "  for i in range(n_filters):\n",
        "    ax = plt.subplot(1,n_filters,i+1)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    plt.imshow(np.array(images[:,:,i] * 255., dtype=np.uint8), cmap='gray')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "print('Filtros aprendidos por la primera capa:')\n",
        "modelConv = Model(inputs=model.input, outputs=model.get_layer(\"conv1\").output)\n",
        "predictions = modelConv.predict(x_train)\n",
        "print(predictions.shape)\n",
        "plot_figures(predictions[0])\n",
        "plot_figures(predictions[1])\n",
        "plot_figures(predictions[2])\n",
        "\n",
        "print('Filtros aprendidos por la segunda capa:')\n",
        "modelConv = Model(inputs=model.input, outputs=model.get_layer(\"conv2\").output)\n",
        "predictions = modelConv.predict(x_train)\n",
        "print(predictions.shape)\n",
        "plot_figures(predictions[0])\n",
        "plot_figures(predictions[1])\n",
        "plot_figures(predictions[2])\n",
        "\n",
        "\n",
        "print('Valores del primer filtro aprendido para la segunda capa:')\n",
        "print( model.get_layer(\"conv2\").get_weights()[0][:,:,0,0] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmz4Gawb9l-w"
      },
      "source": [
        "## Diseño de la red\n",
        "\n",
        "* Mediante las clases de tf.Keras que hemos visto (`Sequential, Conv2D, Flatten, MaxPooling2D, Dropout, Dense`) podemos diseñar la red como nosotros queramos.\n",
        "\n",
        "* Hay que recordar que en la capa de entrada es necesario indicar la forma de los datos de entrada con `input_shape`.\n",
        "\n",
        "* De esta forma podremos añadir más capas, variar el número de filtros, el tamaño de los kernels, etc.\n",
        "\n",
        "* Por ejemplo, podemos crear una red con más capas intercalando MaxPooling y Dropout cada 2 convoluciones, y utilizar más filtros con distintos tamaños de kernel en cada capa:\n",
        "\n",
        "```\n",
        "   model = Sequential()\n",
        "\n",
        "   model.add(Conv2D(64, (5,5), activation='relu', input_shape=(28,28,1)))\n",
        "   model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "   model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "   model.add(Dropout(0.2))\n",
        "\n",
        "   model.add(Conv2D(128, (5,5), activation='relu'))\n",
        "   model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "   model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "   model.add(Dropout(0.2))\n",
        "\n",
        "   model.add(Flatten())\n",
        "   model.add(Dense(128, activation='relu'))\n",
        "   model.add(Dropout(0.2))\n",
        "   model.add(Dense(10, activation='softmax'))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4msg5AQsFw_E"
      },
      "source": [
        "## Aumentado de datos\n",
        "\n",
        "* Como ya vimos previamente, cuando tenemos pocos datos o la variabilidad de estos datos es reducida, podemos utilizar la técnica de aumentado de datos (***data augmentation***).\n",
        "\n",
        "* Esta técnica consiste en generar más datos de entrenamiento a partir de los datos ya existentes.\n",
        "\n",
        "* Para esto se aplican transformaciones sobre las muestras de entrenamiento, como rotaciones, escalado, desplazamientos, flips (dar la vuelta), cambios de color, añadiendo ruido o suavizado, etc.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![Aumentado de datos](http://www.dlsi.ua.es/~jgallego/deepraltamira/data_augmentation1.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "* En tf.Keras podemos utilizar la clase [ImageDataGenerator](https://keras.io/api/preprocessing/image/#imagedatagenerator-class) para generar en tiempo real muestras de entrenamiento con transformaciones aleatorias.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "&#10158; A continuación vamos a ver un código de ejemplo:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn3quFIji6oa"
      },
      "source": [
        "\"\"\"\n",
        "Importamos la clase ImageDataGenerator y llamamos a fit_generator...\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=25)  # Solo aplicamos rotaciones\n",
        "\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    steps_per_epoch=len(x_train) / 32, epochs=1)\n",
        "\n",
        "\n",
        "# Vamos a mostrar algunas de las imágenes que genera\n",
        "print('\\nTransformaciones generadas sobre un dígito:')\n",
        "plt.figure(figsize=(1.5 * 11, 1.5))\n",
        "for idx, img_batch in enumerate(datagen.flow(np.array([x_train[2]]), batch_size=1)):\n",
        "  ax = plt.subplot(1, 11, idx+1)\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  plt.imshow(img_batch[0,:,:,0], cmap='gray')\n",
        "  if idx > 9:\n",
        "    break\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIcOkOuhe_X_"
      },
      "source": [
        "## CNN Hall of Fame\n",
        "\n",
        "* Año tras año la potencia y precisión de las redes CNN ha ido mejorando.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![ImageNet contest](http://www.dlsi.ua.es/~jgallego/deepraltamira/imagenet_contest.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "* Mejorar la precisión o acierto de las redes no siempre quiere decir añadir más capas o más parámetros.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![Redes](http://www.dlsi.ua.es/~jgallego/deepraltamira/applications.png)\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BspVY3a6zAK6"
      },
      "source": [
        "### Modelos pre-entrenados en tf.Keras\n",
        "\n",
        "* tf.Keras incluye la implementación de algunas de las redes más utilizadas [https://keras.io/applications/](https://keras.io/applications/)\n",
        "\n",
        "* Además incluye pesos \"pre-entrenados\" para ImageNet, lo que nos permite utilizar estas redes directamente o aplicar un proceso de \"fine-tuning\" para ajustar los pesos a nuestra base de datos.\n",
        "\n",
        "<br>\n",
        "\n",
        "![Modelos pre-entrenados en tf.Keras](http://www.dlsi.ua.es/~jgallego/deepraltamira/fig_keras_apps_acc.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "&#10158; A continuación se incluye un ejemplo de código de cómo podemos utilizar una de estas redes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI8-yb6Oz9Yu"
      },
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Descargamos una imagen de prueba\n",
        "!wget -q http://www.dlsi.ua.es/~jgallego/deepraltamira/elefante.jpg\n",
        "\n",
        "\n",
        "# Cargamos la imagen\n",
        "img = image.load_img('elefante.jpg', target_size=(299, 299))\n",
        "\n",
        "\n",
        "# Mostramos la imagen\n",
        "plt.imshow(img)\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# La preprocesamos\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "print('Dimensiones de la imagen:', x.shape)\n",
        "\n",
        "\n",
        "# Cargamos el modelo y lo utilizamos para predecir la clase de la imagen\n",
        "model = InceptionV3()\n",
        "preds = model.predict(x)\n",
        "\n",
        "\n",
        "# Decodificamos el resultado, que tendrá el formato (id clase, nombre clase, probabilidad)\n",
        "print('Predicción:', decode_predictions(preds, top=3)[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VjwtbQoD1Aq"
      },
      "source": [
        "## API funcional\n",
        "\n",
        "* Hasta ahora todos los modelos que hemos visto los hemos creado utilizando la clase `Sequential`.\n",
        "\n",
        "* Pero esta clase, como su propio nombre indica, solo permite crear modelos secuenciales.\n",
        "\n",
        "* Para crear modelos más complejos, con múltiples entradas o salidas, o con diseños tipo grafo, podemos utilizar la **[API funcional de tf.Keras](https://keras.io/getting-started/functional-api-guide/)**.\n",
        "\n",
        "* Al utilizar la API funcional cada capa actuará como una función, que recibirá una (o varias) entradas y devolverá un resultado.\n",
        "\n",
        " * Nota: Todas las clases (Conv2D, etc.) que hemos visto hasta ahora se puede utilizar igual.\n",
        "\n",
        "* Por ejemplo, el siguiente modelo secuencial:\n",
        "\n",
        "```\n",
        "   model = Sequential()\n",
        "   model.add(Dense(64, activation='relu', input_dim=784))\n",
        "   model.add(Dense(64, activation='relu'))\n",
        "   model.add(Dense(10, activation='softmax'))\n",
        "```\n",
        "\n",
        "* Utilizando la API funcional se definiría como:\n",
        "\n",
        "```\n",
        "   input = Input(shape=(784,))\n",
        "   x = Dense(64, activation='relu')(input)\n",
        "   x = Dense(64, activation='relu')(x)   \n",
        "   output = Dense(10, activation='softmax')(x)\n",
        "   model = Model(inputs=input, outputs=output)\n",
        "```\n",
        "\n",
        "* De esta forma podremos combinar capas, utilizando además los [operadores de combinación](https://keras.io/layers/merge/) que proporciona tf.Keras (que nos permitirán sumar, restar, concatenar, etc.)\n",
        "\n",
        "* Por ejemplo, para crear una conexión residual (como en la red ResNet):\n",
        "\n",
        "```\n",
        "   x1 = Conv2D(3, (3, 3), padding='same')(x0)\n",
        "   ...\n",
        "   ...\n",
        "   x10 = Conv2D(3, (3, 3), padding='same')(x9)\n",
        "   x11 = tf.keras.layers.add([x1, x10])\n",
        "```\n",
        "\n",
        "* O para crear un módulo Inception (como en la red Inception):\n",
        "\n",
        "```\n",
        "   t1 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\n",
        "   t1 = Conv2D(64, (3, 3), padding='same', activation='relu')(t1)\n",
        "\n",
        "   t2 = Conv2D(64, (1, 1), padding='same', activation='relu')(input_img)\n",
        "   t2 = Conv2D(64, (5, 5), padding='same', activation='relu')(t2)\n",
        "\n",
        "   t3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_img)\n",
        "   t3 = Conv2D(64, (1, 1), padding='same', activation='relu')(t3)\n",
        "\n",
        "   output = tf.keras.layers.concatenate([t1, t2, t3], axis=1)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYjxF5MiyvL1"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**[&#10158;  Vamos a practicar &#10158; ](https://colab.research.google.com/drive/1jMnjMZ85wiNeUkyoUSDWUMwLLlgDsJjg)**\n",
        "\n",
        "---"
      ]
    }
  ]
}