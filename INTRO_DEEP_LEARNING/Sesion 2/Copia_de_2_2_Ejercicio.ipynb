{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap8lgQXsmwM6"
      },
      "source": [
        "# 2.2. Ejercicio\n",
        "\n",
        "En este ejercicio vamos a crear una Red Neuronal Convolucional para clasificar la base de datos llamada **Fashion MNIST**.\n",
        "\n",
        "En primer lugar descargaremos la base de datos y la prepararemos para el entrenamiento. Para practicar vamos a limitar el número de muestras de entrenamiento a 50, comprobaremos el resultado obtenido e intentaremos solucionarlo de distintas formas.\n",
        "\n",
        "Por último entrenaremos usando todos los datos originales de la base de datos y crearemos un formulario para probar cómo funciona la red al clasificar una imagen externa, subida por nosotros.\n",
        "\n",
        "Sigue los pasos indicados y completa las líneas marcadas con \"**TODO**\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxaxLYapw5CE"
      },
      "source": [
        "## Paso 1. Descargar la base de datos\n",
        "\n",
        "En primer lugar descargamos la base de datos y mostramos algunas imágenes escogidas aleatoriamente con su etiqueta o clase correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCyyKW8imv4H"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import                             # TODO - importa la librería de Tensorflow como \"tf\"\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "tf.random.set_seed(2)  # Fijamos la semilla de TF\n",
        "np.random.seed(2)  # Fijamos la semilla\n",
        "\n",
        "\n",
        "# Descargamos la base de datos\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "labels = ['Camiseta/Top', 'Pantalón', 'Suéter', 'Vestido', 'Abrigo', 'Sandalia', 'Camisa', 'Zapatilla', 'Bolso/a', 'Botín']\n",
        "\n",
        "\n",
        "# Mostramos algunas imágenes\n",
        "n = 15\n",
        "index = np.random.randint(len(x_train), size=n)\n",
        "plt.figure(figsize=(n*1.5, 1.5))\n",
        "for i in np.arange(n):\n",
        "    ax = plt.subplot(1,n,i+1)\n",
        "    ax.set_title( labels[ y_train[ index[i] ] ] )\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    plt.imshow(x_train[index[i]], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Mostramos la forma de los datos\n",
        "print('Datos para entrenamiento:')\n",
        "print(' - x_train: {}'.format(  )) # TODO: Muestra la forma de la variable x_train\n",
        "print(' - y_train: {}'.format(  )) # TODO: Muestra la forma de la variable y_train\n",
        "print('Datos para evaluación:')\n",
        "print(' - x_test: {}'.format(  ))   # TODO: Muestra la forma de la variable x_test\n",
        "print(' - y_test: {}'.format(  ))   # TODO: Muestra la forma de la variable y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sed6kT0zxYb9"
      },
      "source": [
        "## Paso 2. Preparar los datos para el entrenamiento\n",
        "\n",
        "En este paso vamos a preparar los datos para el entrenamiento. Sobre las características (x_train y x_test) tendremos que realizar una redimensión para añadir el canal de gris, transformar los datos a decimal y normalizarlos entre 0 y 1. Y las etiquetas las tendremos que transformar a modo categórico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMUewpGzqS8b"
      },
      "source": [
        "# ---------------------\n",
        "def prepare_data(x):\n",
        "  x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)    # Redimensionamos para añadir el canal\n",
        "  x       # TODO: Transforma la variable \"x\" a decimal\n",
        "  x       # TODO: Normaliza la variable \"x\" entre 0 y 1\n",
        "  return x\n",
        "\n",
        "x_train = prepare_data(x_train)\n",
        "x_test  = prepare_data(x_test)\n",
        "\n",
        "\n",
        "# Transformamos las etiquetas a categórico (one-hot)\n",
        "NUM_LABELS = 10\n",
        "y_train =    # TODO: Transforma la variable \"y_train\" a categórica\n",
        "y_test =     # TODO: Transforma la variable \"y_test\" a categórica\n",
        "\n",
        "\n",
        "# Para los primeros ejemplos vamos a limitar el número de imágenes de\n",
        "# entrenamiento a 50. Además nos guardamos un backup con todas las imágenes.\n",
        "x_train_backup = x_train.copy()\n",
        "y_train_backup = y_train.copy()\n",
        "x_train = x_train[:50]\n",
        "y_train = y_train[:50]\n",
        "\n",
        "\n",
        "# Mostramos (de nuevo) las dimensiones de los datos\n",
        "print('Datos para entrenamiento:')\n",
        "print(' - x_train: {}'.format(  )) # TODO: Muestra la forma de la variable x_train\n",
        "print(' - y_train: {}'.format(  )) # TODO: Muestra la forma de la variable y_train\n",
        "print('Datos para evaluación:')\n",
        "print(' - x_test: {}'.format(  ))   # TODO: Muestra la forma de la variable x_test\n",
        "print(' - y_test: {}'.format(  ))    # TODO: Muestra la forma de la variable y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0JJ5_aNxtQh"
      },
      "source": [
        "## Paso 3. Construimos la red y la entrenamos\n",
        "\n",
        "En este tercer paso vamos a construir una red CNN básica y entrenarla. La red estará formada solamente por una capa convolucional (con 1 filtro de tamaño 3x3) seguida por una operación de Max Pooling (de tamaño 2x2), y por último una capa Fully Connected para la salida con 10 neuronas con activación tipo SoftMax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvEjFIf6sHWo"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "model1 =   # TODO: Define un modelo secuencial\n",
        "\n",
        "# Capa convolucional con 1 filtro de tamaño 3x3 seguida de un MaxPooling de 2x2\n",
        "model1.add(Conv2D(1, (3, 3), activation='relu', input_shape=x_train.shape[1:]))\n",
        "model1.add(   )  # TODO: Añade la capa de MaxPooling2D con la configuración indicada\n",
        "\n",
        "# Capa Fully Connected\n",
        "model1.add(Flatten())\n",
        "model1.add(  )  # TODO: Añade una capa densa con \"NUM_LABELS\" neuronas de salida y función de activación SoftMax\n",
        "\n",
        "print(  )       # TODO: Imprime el resumen con la configuración de la red\n",
        "\n",
        "\n",
        "# Compilamos la red\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
        "\n",
        "# Entrenamos durante 10 épocas con un batch de 32\n",
        "history = model1.fit(x_train, y_train,\n",
        "                     validation_data=(x_test, y_test),\n",
        "                     batch_size= ,   # TODO - Asigna un tamaño de batch de 32\n",
        "                     epochs= ,       # TODO - Asigna 10 épocas\n",
        "                     verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsgHGsp1-Y_j"
      },
      "source": [
        "A continuación vamos a mostrar las curvas de aprendizaje y a evaluar el modelo entrenado con los datos de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX9jaJfW4Kgg"
      },
      "source": [
        "# -----------------------------\n",
        "def plot_learning_curves(hist):\n",
        "  plt.plot(hist.history['loss'])\n",
        "  plt.plot(hist.history['val_loss'])\n",
        "  plt.title('Curvas de aprendizaje')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Conjunto de entrenamiento', 'Conjunto de validación'], loc='upper right')\n",
        "  plt.show()\n",
        "\n",
        "plot_learning_curves(history)\n",
        "\n",
        "# Evaluamos usando el test set\n",
        "score =  model1.   # TODO: Llama a la función de evaluación del modelo entrenado con los datos x_test y y_test\n",
        "\n",
        "print('Resultado en el test set:')\n",
        "print('Test loss: {:0.4f}'.format(score[0]))\n",
        "print('Test accuracy: {:0.2f}%'.format(score[1] * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU1DsHXl_KRg"
      },
      "source": [
        "Como se puede ver en los resultados anteriores, parece que el modelo está haciendo overfitting: la varianza, diferencia entre el error de entrenamiento y el de validación, es muy alta. Esto también se puede ver en el accuracy obtenido, 46% para el conjunto de entrenamiento y 30% para la validación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CVqJRNF-Ysj"
      },
      "source": [
        "## Paso 4. Construimos un nuevo modelo de red\n",
        "\n",
        "Como el resultado obtenido con el modelo anterior no es muy bueno y además parece que está haciendo overfitting, vamos a crear otro modelo de red con más filtros por cada capa convolucional, y además le añadiremos un 20% de dropout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5KtERMe-ccX"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "# Capa convolucional con 64 filtros de tamaño 3x3 seguida de un MaxPooling de 2x2\n",
        "model2.add(Conv2D( , (3, 3), activation='relu', input_shape=x_train.shape[1:]))   # TODO: Establece el número de filtros a 32\n",
        "model2.add(  )   # TODO: Añade la capa de MaxPooling2D con la configuración indicada\n",
        "model2.add(  )   # TODO: Añade un Dropout de 0.2\n",
        "\n",
        "# Capa convolucional con 32 filtros de tamaño 3x3 seguida de un MaxPooling de 2x2\n",
        "model2.add(Conv2D( , (3, 3), activation='relu'))   # TODO: Establece el número de filtros a 32\n",
        "model2.add(  )   # TODO: Añade la capa de MaxPooling2D con la configuración indicada\n",
        "model2.add(  )   # TODO: Añade un Dropout de 0.2\n",
        "\n",
        "# Capa Fully Connected\n",
        "model2.add(  )  # TODO: Añade una capa tipo Flatten\n",
        "model2.add(Dense(NUM_LABELS, activation='softmax'))\n",
        "\n",
        "print(model2.summary())\n",
        "\n",
        "# Compilamos y entrenamos\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
        "\n",
        "history = model2.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "                     batch_size=  ,  # TODO: Establece el tamaño de batch a 32\n",
        "                     epochs=  ,      # TODO: Establece el número de épocas a 10\n",
        "                     verbose=1)\n",
        "\n",
        "\n",
        "plot_learning_curves(history)\n",
        "\n",
        "# Evaluamos usando el test set\n",
        "score = model2.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Resultado en el test set:')\n",
        "print('Test loss: {:0.4f}'.format(score[0]))\n",
        "print('Test accuracy: {:0.2f}%'.format(score[1] * 100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TNUGxN7V_8T"
      },
      "source": [
        "Con esta nueva topología de red hemos conseguido mejorar el resultado, además la varianza obtenida es mucho más baja (ya no está haciendo overfitting). Sin embargo, el error de entrenamiento (el bias) sigue siendo bastante alto.\n",
        "\n",
        "Para solucionar esto podríamos aplicar aumentado de datos (este ejercicio se deja como opcional) o añadir más datos al conjunto de entrenamiento. Como inicialmente habíamos limitado los datos vamos a usar la segunda estrategia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AitErqPSWCrp"
      },
      "source": [
        "## Paso 5. Entrenar con todas las imágenes\n",
        "\n",
        "En este paso vamos a restaurar todas las imágenes de entrenamiento que nos habíamos guardado al principio en las variables `x_train_backup` y `y_train_backup`, y volveremos a lanzar el entrenamiento para el segundo modelo de red que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubnM510I_kyU"
      },
      "source": [
        "# Restauramos todas las imágenes de entrenamiento\n",
        "x_train = x_train_backup\n",
        "y_train = y_train_backup\n",
        "\n",
        "print('Datos para entrenamiento:')\n",
        "print(' - x_train: {}'.format( x_train.shape ))\n",
        "print(' - y_train: {}'.format( y_train.shape ))\n",
        "\n",
        "\n",
        "# Iniciamos el entrenamiento\n",
        "history = model2.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
        "                     batch_size=  ,  # TODO: Establece el tamaño de batch a 32\n",
        "                     epochs=  ,      # TODO: Establece el número de épocas a 10\n",
        "                     verbose=1)\n",
        "\n",
        "plot_learning_curves(  )   # TODO: Pasa como parámetro a la función la variable que almacena las curvas de aprendizaje\n",
        "\n",
        "\n",
        "# Evaluamos usando el test set\n",
        "score = model2.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Resultado en el test set:')\n",
        "print('Test loss: {:0.4f}'.format(score[0]))\n",
        "print('Test accuracy: {:0.2f}%'.format(score[1] * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Of5dY9aCzx"
      },
      "source": [
        "Como se puede ver, al entrenar con muchos más datos el resultado obtenido ha mejorado hasta alcanzar el 90% de acierto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDFF0_04WIsX"
      },
      "source": [
        "## Paso 6. Predicción con imágenes externas\n",
        "\n",
        "Por último vamos a probar el funcionamiento del segundo modelo entrenado para la predicción de la clase de imágenes externas, subidas por nosotros mediante un formulario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulYpaH2YAriv"
      },
      "source": [
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "import cv2\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  img = cv2.imread(fn, cv2.IMREAD_COLOR)\n",
        "  img = 255 - cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "  plt.imshow(img)\n",
        "  plt.grid(False)\n",
        "  plt.show()\n",
        "\n",
        "  # Escalamos la imagen\n",
        "  img = cv2.resize(img, (28, 28))\n",
        "\n",
        "  # Normalizamos los datos\n",
        "  img = prepare_data(np.array([img]))\n",
        "\n",
        "  # Ejecutamos la red\n",
        "  prediction = model2.   # TODO: Llama a la función para calcular la predicción a partir de la variable de entrada \"img\"\n",
        "\n",
        "  print('Predicción: ', labels[np.argmax(prediction)])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}