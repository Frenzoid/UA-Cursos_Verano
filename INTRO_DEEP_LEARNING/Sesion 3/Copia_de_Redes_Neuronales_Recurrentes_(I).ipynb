{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTAhhRnfwkl9"
      },
      "source": [
        "# Sesión 3.2. Teoría - Redes Neuronales Recurrentes\n",
        "Curso 2022-23\n",
        "\n",
        "Profesor: [Jorge Calvo Zaragoza](mailto:jcalvo@dlsi.ua.es)\n",
        "\n",
        "## Resumen\n",
        "En esta sesión:\n",
        "  * Introducimos las redes neuronales recurrentes (RNN).\n",
        "  * Veremos el uso de RNN en Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAnBa-KAoDXd"
      },
      "source": [
        "## Introducción\n",
        "  \n",
        "* Determinados problemas no se pueden resolver usando solamente la información del instante actual.\n",
        "\n",
        "* Por ejemplo, qué está sucediendo en una película viendo solamente una imagen, el rol o significado de una palabra sin consultar su contexto, o más sencillo, el siguiente valor de una serie basándonos solamente en el valor anterior.\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "![Serie temporal](http://www.dlsi.ua.es/~jgallego/deepraltamira/serie_temporal.png)\n",
        "\n",
        ".\n",
        "\n",
        "* La redes neuronales tradicionales no pueden hacer esto: sólo condicionan su salida a la entrada.\n",
        "  * Es decir, para una misma entrada, darán siempre la misma salida.\n",
        "\n",
        "* Para solucionarlo surgieron las Redes Neuronales Recurrentes (RNN).\n",
        "\n",
        "* La primera red neuronal recurrente fue propuesta por [Jordan en 1986](https://eric.ed.gov/?id=ED276754).\n",
        "* Posteriormente, en [1997, Hochreiter \ty Schmidhuber](http://www.bioinf.jku.at/publications/older/2604.pdf) propusieron las neuronas tipo LSTM, consideradas clave en el despegue definitivo del Deep Learning para procesamiento de secuencias (y, concretamente, procesamiento de lenguaje natural).\n",
        "\n",
        ".\n",
        "\n",
        "![Deep Learning Timeline](http://www.dlsi.ua.es/~jgallego/deepraltamira/deep_learning_timeline_rnn.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv21sP2TwPON"
      },
      "source": [
        "## Neuronas recurrentes\n",
        "\n",
        "* Las neuronas recurrentes son similares a las neuronas normales pero con **bucles recurrentes** que les permiten mantener un **estado** en el tiempo.\n",
        "\n",
        "* Esta neurona predice su salida ($h_t$) a partir de la entrada en ese instante ($x_t$) y también a partir de la salida del estado anterior ($h_{t-1}$).\n",
        "\n",
        "--\n",
        "\n",
        "![Neurona recurrente](http://www.dlsi.ua.es/~jgallego/deepraltamira/rnn2.png)\n",
        "\n",
        "--\n",
        "\n",
        "* El estado interior de la neurona va cambiando o evolucionando en función de lo que ha visto hasta ese momento, transmitiendo la información que hace falta para el problema de aprendizaje en cuestión.\n",
        "\n",
        "* También podemos pensar en las neuronas recurrentes de forma \"desplegada\", en la que tendríamos tantas neuronas como elementos de la secuencia y cada una le pasa su estado a la siguiente.\n",
        "\n",
        "--\n",
        "\n",
        "![RNN unrolled](http://www.dlsi.ua.es/~jgallego/deepraltamira/rnn_unrolled1.png)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIRbBxDHLngo"
      },
      "source": [
        "## Formulaciones\n",
        "\n",
        "\n",
        "* Con estas neuronas se pueden formular problemas de distinta naturaleza.\n",
        "\n",
        ".\n",
        "\n",
        "![Esquemas con RNN](http://www.dlsi.ua.es/~jgallego/deepraltamira/rnn_diagrams.png)\n",
        "\n",
        ".\n",
        "* Clasificándolos según su entrada y su salida tendríamos:\n",
        "    * **Uno a uno:** clasificación o regresión convencional en el cual a una entrada le corresponde una salida.\n",
        "    * **Muchos a uno:** clasificación de secuencias, en las cuales queremos asignar una categoria a una secuencia. Ejemplo: análisis de sentimiento de un texto.\n",
        "    * **Uno a muchos:** tareas en las cuáles se quiere obtener una secuencia a partir de una única entrada. Ejemplo: descripción automática de imagen.  \n",
        "    * **Muchos a muchos (desacopladas o no):** tareas en las cuales tanto la entrada como la salida son secuencias. Se puede plantear de diferente forma dependiendo de si las secuencias son desacopladas (traducción automática) o si hay una fuerte relación entre cada elemento de la entrada y la salida (etiquetado gramatical).\n",
        "\n",
        "  \n",
        " * En las sesiones anteriores hemos visto la formulación \"Uno a uno\".\n",
        "\n",
        " * A continuación vamos a ver la formulación \"Muchos a uno\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzxtYO5J7EYQ"
      },
      "source": [
        "## Problema de las dependencias a largo plazo\n",
        "\n",
        "* En algunas tareas solo es necesario utilizar la información más reciente para calcular la siguiente predicción.\n",
        "\n",
        "  Es el caso de las series temporales solo necesitamos consultar los $n$ elementos anteriores. O para aprender a predecir la siguiente palabra en una frase sencilla, como por ejemplo \"*Las nubes están en el  [ ?  ]*\"\n",
        "  \n",
        ".\n",
        "\n",
        "![short term depdencies](http://www.dlsi.ua.es/~jgallego/deepraltamira/rnn_shorttermdepdencies1.png)\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "* Pero a veces sí que es necesario consultar más contexto para obtener la siguiente predicción.\n",
        "\n",
        "  Por ejemplo, para predecir la siguiente palabra en la frase \"*Nací en Francia pero a los 12 años me vine a España, ... por eso es que hablo tan bien el [ ? ]*\".\n",
        "  \n",
        "  El contexto reciente sugiere que la siguiente palabra es un idioma pero para saber el idioma correcto tenemos que utilizar la información del principio de la frase.\n",
        "\n",
        ".\n",
        "\n",
        "![long term dependencies](http://www.dlsi.ua.es/~jgallego/deepraltamira/rnn_longtermdependencies1.png)\n",
        "\n",
        ".\n",
        "\n",
        "* Este es un problema para las RNN, dado que según aumenta la distancia hasta la información a utilizar, se vuelve más difícil aprender a conectar la información.\n",
        "\n",
        "* Cuando la información hay que propagarla *muy atrás en el tiempo*, el gradiente de las funciones de activación tiende a 0. Esto se conoce como *vanishing gradient* y constituye uno de los problemás típicos de las redes neuronales.\n",
        "\n",
        "* Para lidiar con esta cuestión surgieron las neuronas tipo LSTM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aiz_huP921-O"
      },
      "source": [
        "## Neuronas LSTM (Long Short-Term Memory)\n",
        "\n",
        "* Su diseño permite *recordar* información tanto a corto como a largo plazo.\n",
        "\n",
        "* La principal diferencia con las neuronas recurrentes es que incluyen (internamente) una celda o bucle de memoria.\n",
        "\n",
        "* Gracias a esto reducen el efecto del \"*vanishing gradient*\" durante el entrenamiento.\n",
        "\n",
        ".\n",
        "\n",
        "![Neurona LSTM](http://www.dlsi.ua.es/~jgallego/deepraltamira/lstm.png)\n",
        "\n",
        ".\n",
        "\n",
        "* Las LSTM incluyen una serie de puertas que permiten controlar la información que entra y sale de la celda de memoria:\n",
        "\n",
        " * **Puerta de entrada:** controla los valores de entrada que se van a utilizar para actualizar el estado de la memoria.\n",
        " * **Puerta de salida:** controla los valores a devolver a partir de la entrada y del contenido de la memoria.\n",
        " * **Puerta de olvido:** controla el mantenimiento del contenido de la memoria.\n",
        "\n",
        "* Existe una versión simplificada de estas neuronas llamadas GRU (Gated Recurrent Units), sin memoria interna y con menos puertas, y por lo tanto con menos parámetros a aprender.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTmYw8WMRKk4"
      },
      "source": [
        "## RNN con Keras\n",
        "\n",
        "* Con Keras podemos crear RNN usando la clase `LSTM`.\n",
        "\n",
        "* La clase [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) permite añadir capas con neuronas de este tipo.\n",
        "\n",
        "  Como parámetro recibe el número de neuronas a, por ejemplo \"`LSTM(32)`\".\n",
        "  \n",
        "  Existen otros parámetros importantes que iremos viendo durante el curso.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpoW_qWjJSPn"
      },
      "source": [
        "### Formato de los datos para Keras\n",
        "\n",
        "* Para entrenar y validar las redes recurrentes tenemos que formatear los datos en secuencias.\n",
        "\n",
        "* Para la arquitectura \"muchos a uno\", cada muestra o ejemplo de entrenamiento constará de una secuencia y de una etiqueta, que se calcula en la salida del último estado de la red recurrente.\n",
        "\n",
        "  Por ejemplo:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{matrix}\n",
        "X & & Y \\\\\n",
        "\\begin{bmatrix}\n",
        "  \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\\\\n",
        "  \\begin{bmatrix} 2 \\\\ 3 \\\\ 4 \\end{bmatrix} \\\\\n",
        "  \\begin{bmatrix} 3 \\\\ 4 \\\\ 5 \\end{bmatrix}  \\\\\n",
        "  \\begin{bmatrix} 4 \\\\ 5 \\\\ 1 \\end{bmatrix}\n",
        "\\end{bmatrix}\n",
        "& &\n",
        "\\begin{bmatrix}\n",
        "\\\\ \\\\ 4 \\\\\n",
        "\\\\ \\\\ 5 \\\\\n",
        "\\\\ \\\\ 1 \\\\\n",
        "\\\\ \\\\ 2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{matrix}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "* Las dimensiones de estas matrices serían:\n",
        "\n",
        "          X = (4, 3)\n",
        "          Y = (4)\n",
        "\n",
        "\n",
        "* Pero para implementar este tipo de redes en Keras (RNN para secuencias temporales) se espera recibir como entrada un vector de **3 dimensiones** y como salida uno de 2. Recordad que la última dimension hace referencia al tamaño de las características, que hay que añadir incluso aunque sea **1** (como ocurre con los canales de una imagen en escala de grises).\n",
        "\n",
        "* Por tanto, para las X tendríamos:\n",
        " * En la primera dimensión el número de ejemplos.\n",
        " * En la segunda dimensión la longitud de la secuencia.\n",
        " * Y en la tercera dimensión las características, que en este caso es un único valor numérico.\n",
        "* Para las Y solo se necesitan 2 dimensiones: la primera con el número de ejemplos y la segunda para el valor a predecir.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{matrix}\n",
        "X & & Y \\\\\n",
        "\\begin{bmatrix}\n",
        "  \\begin{bmatrix} [1] \\\\ [2] \\\\ [3] \\end{bmatrix} \\\\\n",
        "  \\begin{bmatrix} [2] \\\\ [3] \\\\ [4] \\end{bmatrix} \\\\\n",
        "  \\begin{bmatrix} [3] \\\\ [4] \\\\ [5] \\end{bmatrix}  \\\\\n",
        "  \\begin{bmatrix} [4] \\\\ [5] \\\\ [1] \\end{bmatrix}\n",
        "\\end{bmatrix}\n",
        "& &\n",
        "\\begin{bmatrix}\n",
        "\\\\ \\\\ [4] \\\\\n",
        "\\\\ \\\\ [5] \\\\\n",
        "\\\\ \\\\ [1] \\\\\n",
        "\\\\ \\\\ [2] \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{matrix}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "* En este caso las dimensiones de estas matrices serían:\n",
        "\n",
        "          X = (4, 3, 1)\n",
        "          Y = (4, 1)\n",
        "\n",
        "\n",
        "* Y en el caso de que utilicemos la codificación \"one-hot\" o categórica, tendríamos:\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{matrix}\n",
        "X & & Y \\\\\n",
        "\\begin{bmatrix}\n",
        "  \\begin{bmatrix}  \n",
        "    \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\end{bmatrix}\n",
        "    \\\\\n",
        "    \\begin{bmatrix} 0 & 1 & 0 & 0 & 0 \\end{bmatrix}\n",
        "    \\\\\n",
        "    \\begin{bmatrix} 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\n",
        "  \\end{bmatrix}\n",
        "  \\\\\n",
        "  \\begin{bmatrix}\n",
        "    \\begin{bmatrix} 0 & 1 & 0 & 0 & 0 \\end{bmatrix}\n",
        "    \\\\\n",
        "    \\begin{bmatrix} 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\n",
        "    \\\\\n",
        "    \\begin{bmatrix} 0 & 0 & 0 & 1 & 0 \\end{bmatrix}\n",
        "  \\end{bmatrix}\n",
        "  \\\\\n",
        "  \\begin{bmatrix}\n",
        "    \\begin{bmatrix} 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\n",
        "    \\\\\n",
        "    \\begin{bmatrix} 0 & 0 & 0 & 1 & 0 \\end{bmatrix}\n",
        "    \\\\\n",
        "    \\begin{bmatrix} 0 & 0 & 0 & 0 & 1 \\end{bmatrix}\n",
        "  \\end{bmatrix}  \n",
        "  \\\\\n",
        "  \\begin{bmatrix}\n",
        "    \\begin{bmatrix} 0 & 0 & 0 & 1 & 0 \\end{bmatrix}\n",
        "    \\\\\n",
        "    \\begin{bmatrix} 0 & 0 & 0 & 0 & 1 \\end{bmatrix}\n",
        "    \\\\\n",
        "    \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\end{bmatrix}\n",
        "  \\end{bmatrix}\n",
        "\\end{bmatrix}\n",
        "& &\n",
        "\\begin{bmatrix}\n",
        "\\\\ \\\\ \\begin{bmatrix} 0 & 0 & 0 & 1 & 0 \\end{bmatrix} \\\\\n",
        "\\\\ \\\\ \\begin{bmatrix} 0 & 0 & 0 & 0 & 1 \\end{bmatrix} \\\\\n",
        "\\\\ \\\\ \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\end{bmatrix} \\\\\n",
        "\\\\ \\\\ \\begin{bmatrix} 0 & 1 & 0 & 0 & 0 \\end{bmatrix} \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{matrix}\n",
        "\\end{equation}\n",
        "\n",
        "* En este caso las dimensiones serían:\n",
        "\n",
        "          X = (4, 3, 5)\n",
        "          Y = (4, 5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGT2o9qq2jd5"
      },
      "source": [
        "### Ejemplo de RNN\n",
        "\n",
        "Vamos a implementar nuestra primera red recurrente para ajustarse con los datos del ejemplo anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qButjwa2fEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8fea4a-1e1b-41e9-a7c4-4d3e33d5244c"
      },
      "source": [
        "%%capture --no-stdout\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------\n",
        "# Datos del ejemplo\n",
        "#\n",
        "print()\n",
        "print('Creamos datos...')\n",
        "\n",
        "X = np.asarray([\n",
        "     [ [1,0,0,0,0],\n",
        "       [0,1,0,0,0],\n",
        "       [0,0,1,0,0]\n",
        "     ],\n",
        "     [ [0,1,0,0,0],\n",
        "       [0,0,1,0,0],\n",
        "       [0,0,0,1,0]\n",
        "     ],\n",
        "     [ [0,0,1,0,0],\n",
        "       [0,0,0,1,0],\n",
        "       [0,0,0,0,1]\n",
        "     ],\n",
        "     [ [0,0,0,1,0],\n",
        "       [0,0,0,0,1],\n",
        "       [1,0,0,0,0]\n",
        "     ],\n",
        "    ])\n",
        "\n",
        "Y = np.asarray([\n",
        "      [0,0,0,1,0],\n",
        "      [0,0,0,0,1],\n",
        "      [1,0,0,0,0],\n",
        "      [0,1,0,0,0]\n",
        "    ])\n",
        "\n",
        "\n",
        "print('X: {}'.format(X.shape))\n",
        "print('Y: {}'.format(Y.shape))\n",
        "\n",
        "# ---------------------\n",
        "# Creamos red\n",
        "#\n",
        "print()\n",
        "print('Creamos red...')\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.InputLayer((3, 5))) # Longitud de las secuencias , características\n",
        "model.add(tf.keras.layers.LSTM(32))\n",
        "model.add(tf.keras.layers.Dense(5))                    # Categorías del ejemplo\n",
        "model.add(tf.keras.layers.Activation('softmax'))       # Clasificación\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop')              # RMSProp es típico de RNN\n",
        "model.summary()\n",
        "\n",
        "# ---------------------\n",
        "# Entrenamiento\n",
        "#\n",
        "print()\n",
        "print('Entrenamiento...')\n",
        "\n",
        "model.fit(X,Y,epochs=100,verbose=0)\n",
        "\n",
        "# ---------------------\n",
        "# Datos  del ejemplo\n",
        "#\n",
        "print()\n",
        "print('Predicción...')\n",
        "\n",
        "prediccion = model.predict(X)\n",
        "categorias = np.argmax(prediccion,axis=1)+1\n",
        "\n",
        "for idx_s, secuencia in enumerate(X):\n",
        "  print()\n",
        "  print('Secuencia:')\n",
        "  print(secuencia)\n",
        "  print('Prediccion: {}'.format(categorias[idx_s]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creamos datos...\n",
            "X: (4, 3, 5)\n",
            "Y: (4, 5)\n",
            "\n",
            "Creamos red...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 32)                4864      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 165       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,029\n",
            "Trainable params: 5,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Entrenamiento...\n",
            "\n",
            "Predicción...\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "\n",
            "Secuencia:\n",
            "[[1 0 0 0 0]\n",
            " [0 1 0 0 0]\n",
            " [0 0 1 0 0]]\n",
            "Prediccion: 4\n",
            "\n",
            "Secuencia:\n",
            "[[0 1 0 0 0]\n",
            " [0 0 1 0 0]\n",
            " [0 0 0 1 0]]\n",
            "Prediccion: 5\n",
            "\n",
            "Secuencia:\n",
            "[[0 0 1 0 0]\n",
            " [0 0 0 1 0]\n",
            " [0 0 0 0 1]]\n",
            "Prediccion: 1\n",
            "\n",
            "Secuencia:\n",
            "[[0 0 0 1 0]\n",
            " [0 0 0 0 1]\n",
            " [1 0 0 0 0]]\n",
            "Prediccion: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FafpZSVcHFw"
      },
      "source": [
        "&nbsp;\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        " Vamos a practicar &#10158; [Predicción de los valores de bolsa](https://colab.research.google.com/drive/1-4fHNwYT5UTTQZ5qhN3hpTVei7IwkYEd?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plehsoP1uwBE"
      },
      "source": [
        "### Otras características de las redes recurrentes\n",
        "\n",
        "#### Predicción en cada instante temporal\n",
        "\n",
        "Ya hemos visto que las redes recurrentes son adecuadas para tratar con problemas de naturaleza secuencial.\n",
        "\n",
        "A diferencia de lo visto hasta ahora, también podemos obtener una salida para cada instante temporal mediante un parámetro de la capa recurrente:\n",
        "\n",
        "* **return_sequences**: cuando está desactivado (por defecto), sólo devuelve la salida del último instante de tiempo; cuando está activado, devuelve la salida de todos los instantes.\n",
        "\n",
        "Atención a la salida de la LSTM según se active o no este parámetro:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QLE7WYeuyP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c520294-9df6-490c-d1f6-2673d01a97a3"
      },
      "source": [
        "# La entrada a una red recurrente tiene dos dimensiones:\n",
        "# - El número de instantes de tiempo (T)\n",
        "# - Número de características de entrada (F)\n",
        "import tensorflow as tf\n",
        "\n",
        "T = 100\n",
        "F = 2\n",
        "capa_entrada = tf.keras.layers.Input(shape=(T, F))\n",
        "\n",
        "x0 = tf.keras.layers.LSTM(64)(capa_entrada)\n",
        "x1 = tf.keras.layers.LSTM(64,return_sequences=True)(capa_entrada)\n",
        "\n",
        "print('LSTM: ' + str(x0))\n",
        "print('LSTM + return_sequences: ' + str(x1))\n",
        "print()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM: KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='lstm_3/PartitionedCall:0', description=\"created by layer 'lstm_3'\")\n",
            "LSTM + return_sequences: KerasTensor(type_spec=TensorSpec(shape=(None, 100, 64), dtype=tf.float32, name=None), name='lstm_4/PartitionedCall:1', description=\"created by layer 'lstm_4'\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLPo7eLOu2Q2"
      },
      "source": [
        "Esto permite, por lo tanto, construir una red neuronal para predecir un valor en cada instante temporal.\n",
        "\n",
        "En el este ejemplo entrenamos una red neuronal que nos diga, para cada instante de una secuencia temporal, si el número de entrada es igual al anterior. En este caso, los datos serían:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{matrix}\n",
        "X & & Y \\\\\n",
        "\\begin{bmatrix}\n",
        "  \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix} \\\\\n",
        "  \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\\n",
        "  \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}  \\\\\n",
        "  \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}  \\\\\n",
        "  \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}\n",
        "\\end{bmatrix}\n",
        "& &\n",
        "\\begin{bmatrix}\n",
        "  \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\\n",
        "  \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\\n",
        "  \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}  \\\\\n",
        "  \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}  \\\\\n",
        "  \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n",
        "\\end{bmatrix}\n",
        "\\end{matrix}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "* Las dimensiones de estas matrices serían:\n",
        "\n",
        "          X = (5, 4)\n",
        "          Y = (5, 4)\n",
        "\n",
        "* Y si preparamos las matrices para los lotes de Keras, habría que añadir la dimensión extra de las características:\n",
        "\n",
        "          X = (5, 4, 1)\n",
        "          Y = (5, 4, 1)\n",
        "\n",
        "En esta ocasión, como no tenemos entrada categórica, el propio valor ($0$ o $1$) es suficiente para representar la entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36_z2fh1vQ2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1eee20-c5d0-494b-e464-8d8334a101bb"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Datos del ejemplo\n",
        "#\n",
        "\n",
        "print()\n",
        "print('Creamos datos...')\n",
        "\n",
        "X = np.asarray([\n",
        "       [1,0,1,1],\n",
        "       [0,0,0,1],\n",
        "       [1,1,0,0],\n",
        "       [1,1,1,0],\n",
        "       [0,1,1,0]\n",
        "    ])\n",
        "\n",
        "Y = np.asarray([\n",
        "       [0,0,0,1],\n",
        "       [0,1,1,0],\n",
        "       [0,1,0,1],\n",
        "       [0,1,1,0],\n",
        "       [0,0,1,0]\n",
        "    ])\n",
        "\n",
        "X = np.expand_dims(X,axis=-1) # Añadimos una dimension \"ficticia\" al final\n",
        "Y = np.expand_dims(Y,axis=-1) # Añadimos una dimension \"ficticia\" al final\n",
        "\n",
        "\n",
        "print('X: {}'.format(X.shape))\n",
        "print('Y: {}'.format(Y.shape))\n",
        "\n",
        "# ---------------------\n",
        "# Creamos red\n",
        "#\n",
        "print()\n",
        "print('Creamos red...')\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(4, 1))) # Longitud de las secuencias X características\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))                    # Clasificacion binaria\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop')              # RMSProp es típico de RNN\n",
        "model.summary()\n",
        "\n",
        "# ---------------------\n",
        "# Entrenamiento\n",
        "#\n",
        "print()\n",
        "print('Entrenamiento...')\n",
        "\n",
        "model.fit(X,Y,epochs=500,verbose=0)  # Suficientes épocas para memorizar\n",
        "\n",
        "# ---------------------\n",
        "# Datos  del ejemplo\n",
        "#\n",
        "print()\n",
        "print('Predicción...')\n",
        "\n",
        "prediccion = model.predict(X)\n",
        "\n",
        "for idx_s, secuencia in enumerate(X):\n",
        "  print()\n",
        "  print('Secuencia: {}'.format(np.squeeze(secuencia)))\n",
        "  print('Prediccion: {}'.format(np.squeeze(prediccion[idx_s])))\n",
        "  print('Decision: {}'.format( np.squeeze(prediccion[idx_s]) > 0.5 ))\n",
        "  print('Valor correcto: {}'.format(np.squeeze(Y[idx_s])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creamos datos...\n",
            "X: (5, 4, 1)\n",
            "Y: (5, 4, 1)\n",
            "\n",
            "Creamos red...\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_5 (LSTM)               (None, 4, 64)             16896     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4, 1)              65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,961\n",
            "Trainable params: 16,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            "Entrenamiento...\n",
            "\n",
            "Predicción...\n",
            "1/1 [==============================] - 0s 462ms/step\n",
            "\n",
            "Secuencia: [1 0 1 1]\n",
            "Prediccion: [0.00776608 0.5111109  0.06061182 0.99766654]\n",
            "Decision: [False  True False  True]\n",
            "Valor correcto: [0 0 0 1]\n",
            "\n",
            "Secuencia: [0 0 0 1]\n",
            "Prediccion: [9.8397555e-03 6.0357147e-01 9.8159659e-01 8.9418102e-04]\n",
            "Decision: [False  True  True False]\n",
            "Valor correcto: [0 1 1 0]\n",
            "\n",
            "Secuencia: [1 1 0 0]\n",
            "Prediccion: [0.00776608 0.78016937 0.02937662 0.9950838 ]\n",
            "Decision: [False  True False  True]\n",
            "Valor correcto: [0 1 0 1]\n",
            "\n",
            "Secuencia: [1 1 1 0]\n",
            "Prediccion: [0.00776608 0.78016937 0.92028284 0.00302304]\n",
            "Decision: [False  True  True False]\n",
            "Valor correcto: [0 1 1 0]\n",
            "\n",
            "Secuencia: [0 1 1 0]\n",
            "Prediccion: [9.8397555e-03 3.0800235e-01 9.9938333e-01 1.5397131e-04]\n",
            "Decision: [False False  True False]\n",
            "Valor correcto: [0 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11SlD1svxtRa"
      },
      "source": [
        "Obviamente, este ejemplo tiene poco sentido puesto que la red neuronal memoriza fácilmente. Sin embargo, permite observar cómo modelar un problema de **muchos a muchos**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMNycQGgvR2E"
      },
      "source": [
        "#### Capas recurrentes apiladas\n",
        "\n",
        "Si queremos apilar varias capas recurrentes consecutivas (como se hace con las CNN) necesitamos hacer uso de *return_sequences* también. De esta forma, la neurona en un instante temporal propaga su salida a la siguiente capa. Esto es independiente de si la ultima capa activa o no *return_sequences*.\n",
        "\n",
        ".\n",
        "\n",
        "<img src=\"http://www.dlsi.ua.es/~jcalvo/curso-deep-learning/two-lstm.png\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EktWJ4JJiLYU"
      },
      "source": [
        "Un ejemplo por código, usando el API secuencial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZTx97cRveyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64436ee-3d96-494a-8132-6051bc46b123"
      },
      "source": [
        "%%capture --no-stdout\n",
        "\n",
        "T = 100\n",
        "F = 2\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.InputLayer((T, F)))\n",
        "model.add(tf.keras.layers.LSTM(16,return_sequences=True))\n",
        "model.add(tf.keras.layers.LSTM(16,return_sequences=True))\n",
        "model.add(tf.keras.layers.LSTM(16,return_sequences=True))\n",
        "model.add(tf.keras.layers.LSTM(10))\n",
        "model.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 100, 16)           1216      \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 10)                1080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,630\n",
            "Trainable params: 6,630\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5rNwR7dvhnR"
      },
      "source": [
        "#### Recurrencia bidireccional\n",
        "\n",
        "En la mayoría de tareas, es interesante tener no sólo el contexto anterior sino también el posterior a la hora de predecir un elemento en un instante dado. En tal caso, podemos establecer una **recurrencia bidireccional**, en las cuales una neurona tiene dos conexiones recurrentes que, **a la hora de desplegarse**, lo hacen en direcciones opuestas.\n",
        "\n",
        "![texto alternativo](https://cdn-images-1.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)\n",
        "\n",
        "Para implementar redes bidireccionales Keras aporta un *wrapper* que acepta como parámetro un objeto que implemente la super clase recurrente:\n",
        "\n",
        "* [keras.layers.Bidirectional](https://keras.io/api/layers/recurrent_layers/bidirectional/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d60uxNpvksK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f527c4a7-7c69-49af-c9ba-7b4fe63100a2"
      },
      "source": [
        "T = 100\n",
        "F = 2\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.InputLayer((T, F)))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)))\n",
        "model.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirectiona  (None, 32)               2432      \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,762\n",
            "Trainable params: 2,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92MpeY3lkHk3"
      },
      "source": [
        "**Pregunta**: ¿Cuál es la operación que une las dos salidas individuales de una capa bidireccional?"
      ]
    }
  ]
}